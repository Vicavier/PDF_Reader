第11章 非合作博弈中的协同进化学习

11.1 博弈中协同进化学习简介

无论是在自然界还是在人造环境中，许多问题都涉及到实体需要做出决策。这类决策问题可以被抽象为博弈（game）的形式。或许并不令人意外，发现这类决策问题解的迭代过程与协同进化（coevolution）之间存在着密切的对应关系。考虑一个多智能体系统，该系统由一组智能体组成，这些智能体以成对的方式相互作用。这种交互的环境可以使用特定的博弈进行建模。在两人博弈的情形下，规则限制了博弈智能体能够实施的行为，以保证在交互过程中只做出允许的（合法的）动作。这些决策行为（策略）可以用参数化的形式进行表示。这样，对策略表示及其相关参数的具体选择就规定了博弈的策略集。此外，博弈的目标（目标函数）是已知且明确规定的。在博弈过程中，双方根据其交互（博弈过程）获得以收益（payoff）形式表现的奖励。两位玩家累积的收益共同决定了博弈结束时玩家所获得的博弈结果。例如，大多数竞争性博弈是零和博弈（zero-sum），即一方获得正收益意味着对方获得负收益（例如赢得比赛的一方意味着对方输掉比赛）。无论如何，对博弈的详细描述意味着策略集是已知的（例如在对称博弈中对双方都可用），并且在博弈过程中任意一对玩家所采用的策略的结果可以被确定（即可以像标准型博弈那样进行计算和保存，或通过实际或模拟的博弈过程来计算）[7]。

在多智能体协同进化系统中，智能体的集合构成了一个进化种群。在每一代中，种群中的智能体相互作用，并最终为了繁殖资源展开竞争。具体来说，智能体获得的累积博弈结果，以某种方式决定了其繁殖的频率，即该智能体被选中用于生成具有相同或不同但密切相关策略的子代智能体的频率。实际复制子代智能体的操作涉及到应用变异算子，对父代智能体的策略参数做出具体变更。需要注意的是，具体的参数值不仅描述了智能体所实施策略的内部结构，还与策略的表示形式一起使用，使该策略能够作为输入-输出响应函数来表达和调节其具体的决策行为。该过程在下一代对新种群重复进行，持续足够长的时间，使后续几代中的种群相较于早期种群在某些“优良性”或“最优性”标准下表现更加优越。

对于这种协同进化过程及其产出的智能体种群，可以有两种不同的视角。一种观点是可以将这种模拟的协同进化看作自然进化的过程，关注的是某些策略在策略集合中的进化成功。同样，也可以更加关注个体智能体，将协同进化视为一种学习过程，在该过程中智能体能够自适应地调整其所选择实施的策略。正是在这种观点下，协同进化学习通常被简明地描述为：由一组自适应智能体组成的种群，这些智能体通过完全由其策略交互所驱动的繁殖过程学习行为[10, 11]。
在协同进化学习领域的大量研究致力于理解自然进化过程的机制，并由此开发出允许针对手头问题进行解空间搜索的学习方法和系统，尤其适用于被抽象为对抗性博弈的决策问题。在这一问题求解背景下，协同进化学习旨在鼓励通过迭代过程将繁殖资源分配给代理的策略表示中的参数变更，随着时间（世代）的推移，从而获得一组高性能的博弈策略。这一过程通过协同进化算法（co-evolutionary algorithms, CEAs）实现，后者通过“生成-测试”过程，优化与代理策略表示相关的参数，从而在庞大的策略空间中搜索出高性能博弈策略。该优化过程由进化种群中各代理之间竞争性博弈的结果所引导。当竞争代理组成的进化种群进入持续性的“军备竞赛”（arms race）动态，从而不断生成具备更高性能的策略代理时，协同进化学习便取得了成功。因此，必须特别指出，协同进化学习在本质上与机器学习在对抗性博弈情境中的经典设置是不同的。与监督学习相比，协同进化学习中并未正式使用包含实现测试策略的固定代理的训练样本来评估进化代理的表现。与强化学习相比，协同进化学习中的适应过程仅在博弈结束时进行，此时博弈的最终结果（如胜负）才用于引导代理的适应过程 [7]。

协同进化学习应用于搜索对抗性博弈策略的一个例子涉及各种难度的双人棋类游戏，包括跳棋 [7, 21]、黑白棋 [8, 9]、国际象棋 [22]，甚至围棋 [32]。值得注意的是，这些研究所采用的方法与传统人工智能中解决双人棋类游戏问题的方法是不同的。后者关注基于极大极小（minimax）原理的高效搜索过程，在博弈树中生成两位玩家可能采用的交替移动序列（路径），并最终找到能够使根节点玩家获胜的路径。而协同进化算法虽然仍然使用极大极小搜索，但仅在极浅的层数（如个位数）进行，且通常只是为了确保代理选择的是一个合法的着法。相反，其主要动机是在利用复杂非线性函数进行棋盘评估所生成的庞大战略集中的子集内，搜索表现优异的博弈策略。例如，这些棋盘评估策略的表示选择通常涉及使用人工神经网络（Artificial Neural Networks, ANNs），以捕捉对当前博弈状态棋盘上可能模式进行推理所需的复杂性，并通过选择有利于当前玩家而非对手的着法来给出合理反馈。

关键的是，支持协同进化方法的重要论据之一在于，该方法能够使自适应代理在无需引入大量领域知识的情况下，高水平地学习这些棋类博弈。我们在[8, 9]中的研究表明，协同进化训练的ANN博弈代理在黑白棋中表现出的博弈策略，与其他文献中高级人类玩家在其它棋类博弈中采用的策略表现出惊人的相似性 [21, 22]。
然而，共进化方法不仅仅局限于用于搜索具有一组优势策略的竞争性游戏中的最优博弈策略。在这种问题求解环境下，实施此类优势策略的智能体，无论对手采取何种替代策略，始终能够获得该游戏的最佳结果。然而，这样的优势策略并不一定能够带来最优的收益。换句话说，如果采用其他策略，所有博弈参与者的收益都有可能同时提高 [28]。对于完全竞争的零和游戏而言，这并不是一个问题，因为在这些游戏中，关注点与每个玩家的个人目标一致，即通过确保在与任意替代策略的对抗中获得更高收益，从而最大化获胜结果的策略。在一些非完全竞争性的非合作博弈中，参与者的互动中既包含竞争性成分，也包含合作性成分。在此类非零和博弈中，可以将单个体的目标解释为竞争性目标，而将参与群体的目标解释为合作性目标。诸如迭代囚徒困境（Iterated Prisoner’s Dilemma, IPD）[1–3]等游戏能够模拟某些复杂的现实世界互动，相关研究的主要兴趣涉及最优性或解概念的其他表现形式，如关注全部参与智能体群体整体的表现。在这类环境下，采用多智能体系统与学习驱动的共进化仿真方法，可以进行“假如”分析，从而解答诸如在交互环境中建立哪些条件能够促进自适应智能体学习特定策略等问题 [4, 14, 18–20, 25, 27]。
本章主要关注于共演化学习在非合作博弈中的应用，作为一种仿真模拟，我们在[13]中进行了相关研究。在第11.2节中，我们将首先详细描述IPD（重复囚徒困境）博弈，并简要回顾相关文献，阐述为何该博弈已被广泛用作实际世界各种情境下交互的模型，这些情境涵盖生物学、社会经济、政治及工程领域。随后，我们将介绍一个更为复杂、具有更多选择的IPD博弈构造，以更好地反映现实中的互动过程。第11.3节介绍了我们案例研究[13]的第一部分，旨在探究在IPD博弈中引入共演化学习时，为什么当交互中可选策略变多时，个体会逐渐学习以较低的合作水平进行互动，最终导致背叛行为的发生。至今仍不清楚，为何直接互惠机制（如重复交互）——已被证明能够促进合作策略的共演化学习——在这种情况下会变得不再有效。我们的共演化模拟表明，代理体对中等程度合作选择的意图判断能力减弱，进而进化出以较低合作水平博弈的策略，这些策略在短期视角下能获得更高的收益。实际上，随着选择的增多，代理体有了更多机会去利用其他个体。尽管如此，现实世界中的人类交互显示，合作仍可通过其他机制建立，尤其是涉及间接互动（即声誉机制）时。第11.4节将介绍我们案例研究[13]的第二部分，其中我们将声誉机制引入IPD博弈中的共演化代理体。模拟结果让我们揭示了声誉为何以及如何帮助代理体即便在有更多选择的IPD环境下也能学习合作行为。具体而言，代理体能够利用与其他代理体相关联的声誉分数，快速进入互惠合作的博弈过程。最后，第11.5节将以对不同声誉机制的实现方式及其对这些自适应代理体合作演化影响的讨论为本章作结。

11.2 现实交互下的复杂重复囚徒困境

本节将对IPD博弈进行概述，并简要回顾部分研究，这些研究利用该类数学博弈模型对现实世界的交互过程进行了抽象，以便更深入地理解相关交互中所观察到的各种现象。我们首先介绍经典的IPD博弈，即两个玩家进行重复博弈——在有限的回合内，每回合都需决定采取哪种选择。在经典的IPD博弈中，每名玩家每回合有两个可选动作（合作与背叛）[1–3]。玩家所获得的收益取决于双方在该回合所做的联合选择。对于第一位玩家，其所有收益由下列矩阵给出：

图11.1：两玩家、两选项IPD博弈的收益矩阵。矩阵中列举的是第一名（或行玩家）选取某一动作时的收益。第二名（或列玩家）的收益矩阵可通过对图中所示矩阵取转置获得。
在收益矩阵 $\mathbf{G} = (g_{ij} : i, j = \mathrm{C}, \mathrm{D})$ 中，指标 $i$ 表示（行）第一个玩家的决策，而 $j$ 表示（列）第二个玩家的决策。对于经典的二人两选囚徒困境（IPD）游戏，共有四种可能的联合行动组合，因此对应四个收益值。这里，$\mathbf{G} = (R, S, T, P) = (g_{\mathrm{CC}}, g_{\mathrm{CD}}, g_{\mathrm{DC}}, g_{\mathrm{DD}})$，其中 $R, S, T, P \in \mathbb{R}_{\geq 0}$。这四个收益 $R, S, T, P$ 分别对应：相互合作获得的奖励（$R$），当对方背叛时的“傻瓜收益”（$S$），背叛的“诱惑收益”（$T$），以及相互背叛时的惩罚（$P$）[3]。矩阵 $\mathbf{G}$ 中的收益必须满足以下条件：

(1) $T > R$ 且 $P > S$（背叛带来的回报更高），  
(2) $R > P$（相互合作的回报高于相互背叛），  
(3) $R > \frac{T + S}{2}$（交替行为的平均收益不及持续合作）。

文献[4]采用了 $(R, S, T, P) = (3, 0, 5, 1)$，虽然只要满足IPD条件，也可以使用其他值。在本课题案例中，我们采用 $(R, S, T, P) = (4, 0, 5, 1)$。需要注意的是，第二个玩家的收益矩阵为 $\mathbf{G}^{\mathsf{T}}$，如图11.1所示。

游戏的玩法很简单，通常是同时博弈——即每轮中，两个玩家同时决定此次行动的选择。对于基于记忆的反应型策略，尤其是我们在案例研究中关注的记忆一位的简单反应型策略[12]，每种策略都包括一个赛前档案。这个档案用来启动游戏——即决定首轮的行动。实现方法之一是存储一个联合行动，然后将其作为输入传递给策略的行为响应函数，从而获得决策结果。游戏开始时，两个玩家都做出初始动作，此后使用当前的联合动作生成响应，作为后续的联合动作输入。这样便得到一个有限长度的联合动作序列，可进一步变换为某玩家的有限长度收益序列 $(p_m : m = 1, \ldots, M),\ M \in \mathbb{N}$，其中每个 $p_m \in \{R, S, T, P\}$。

任一玩家的最终收益是其累计收益，例如取所有收益的和或均值 $(1/M)\sum_{m=1}^{M}p_m$。若采用平均收益决胜，则其结果必然位于区间 $[S, T]$ 内。尽管这种非合作博弈的结构非常简单，却能很好地刻画个人理性（即背叛的诱惑）与群体理性（相互合作的奖赏）之间的张力[2]。如果两个玩家选择相互合作而非相互背叛，他们的收益可以被同时提高。然而，这种相互合作也意味着合作方容易被实施“背叛”这一占优策略的合作者所剥削，后者以谋取最大化的个体收益。本游戏中的竞合困境也引发了关于其各种可能解的理性推理方式的进一步思考。
IPD游戏家族已成为研究在个体相互作用过程中，合作元素如何在个体自我最大化效用（收益）的竞争背景下被促进和建立的流行模型。这类游戏在许多研究领域受到广泛关注，用于刻画自然系统中的生物、社会、经济和政治相互作用[3]，以及涉及工程环境中的人工系统。后一类情况在分布式系统中尤为常见，在这种系统中，各实体需要在不同选择之间做出决策，即在因运行需求而产生的、自身最小化成本与整体（交互实体群体）最小化成本之间权衡。事实上，互联网服务提供商（ISP）路由游戏便是这样的一个典型例子[28]。在这里，各参与方的交互可直接在工程背景下解释，多智能体系统代表了典型的通信网络系统，互联了多个服务提供商。在基本的ISP路由游戏中，两个玩家分别代表需要反复相互交流的ISP，即因服务请求导致跨网络进行数据通信。通常，ISP可以选择合作（即使用自身部分网络资源）或背叛（即几乎全部利用对方ISP的网络资源）。ISP游戏的收益矩阵由与路由选择相关的各种成本（网络资源使用量）指定。这些数值可以被适当变换，使其满足前文所述的三条IPD游戏条件。已有许多研究试图确定合作博弈能够形成有效策略的条件[1, 2]。这些条件可能源于对游戏交互的特定规定，或在共同进化过程中实施的具体设计选择。与游戏交互相关条件的一个例子是，限制策略集为具有有限记忆的反应型策略（即策略当前决策基于以往的历史行动）。关于共同进化过程条件的另一个例子是假设代理的交互是完全混合的，即每个代理与种群中所有其他代理交互。已有研究一般表明，从随机的IPD策略种群出发，该种群能够进化为主要采用合作策略。此外，尽管这种合作的进化是不稳定的[6]，但其可以在较长时期内持续存在[14]。对于那些通过计算机模拟研究特定IPD策略在进化环境下为何以及如何取得成功的工作，主要有两类方法。第一类方法采用生态模拟，强调预设策略的频率依赖型复制，更符合经典演化博弈研究[2, 5]。第二类方法则基于协同进化算法（CEAs），用于开发多智能体协同进化学习系统[4, 18–20, 25]。本研究采用第二种方法，因为它为研究自适应代理通过基于策略交互并针对特定系统条件进行适应过程来学习有效策略，提供了理论框架。
大多数早期采用协同进化学习方法研究囚徒困境(IPD)博弈的工作[4, 14, 18–20, 25, 27]仅考虑了“合作”与“背叛”两种选择。一些其它研究则扩展了经典的双选IPD博弈，探讨了包含更多选择、更为复杂的交互情形。对于多选择的IPD，已有两种主要研究方法：(1) 涉及连续合作程度的交互[24]；(2) 涉及多个离散合作水平的交互[15–17]。在第二种方法中，$n$-选择IPD基于经典双选IPD的简单线性插值，采用以下等式[12]：

$$
p_A = 2.5 - 0.5c_A + 2c_B,\quad -1 \leq c_A, c_B \leq 1,
$$

其中，$p_A$为A玩家的收益，$c_A$和$c_B$分别表示A（第一位玩家）和B（第二位玩家）所做决策的合作水平。$c_A, c_B$取值自一个包含$n$个元素的集合$\{\mathrm{choice}_1, \ldots, \mathrm{choice}_n\}$，这一集合对应于闭区间$[-1,1]$上等间距的实数值，其中$\mathrm{choice}_1 = -1$，$\mathrm{choice}_n = 1$。以四选IPD游戏为例，$c_A,c_B \in \{-1,-1/3,1/3,1\}$，其中$-1$表示完全背叛，1表示完全合作。

针对$n$-选择IPD收益矩阵的生成，需满足以下条件[12]：

1. 对于$c_A < c'_A$且$c_B$为常量，有：
   $$
   p_A(c_A, c_B) > p_A(c'_A, c_B)
   $$

2. 对于$c_A \leq c'_A$且$c_B < c'_B$，有：
   $$
   p_A(c_A, c_B) < p_A(c'_A, c'_B)
   $$

3. 对于$c_A < c'_A$且$c_B < c'_B$，有：
   $$
   p_A(c'_A, c'_B) > \frac{1}{2}\big(p_A(c_A, c'_B) + p_A(c'_A, c_B)\big)
   $$

这些条件可类比为双选IPD中的规则：第一条确保背叛总能获得更高收益；第二条保证互惠合作的收益高于互惠背叛；第三条确保轮流进行合作和背叛的收益低于持续合作。

图11.2展示了四选IPD下A玩家的收益矩阵。可注意到，所生成矩阵中的任何$2\times2$子矩阵都能满足上述三条IPD条件。此外，若$c_A, c_B$仅取$\{-1,1\}$两种合作水平，则生成的收益矩阵$(R, S, T, P) = (4, 0, 5, 1)$与经典IPD游戏中使用的是一致的。

所有基于协同进化的多选择IPD研究[15–17, 24]普遍采用人工神经网络（ANN）作为主体策略的主要表示方式。总体来说，相关研究结论表明，若主体能实施多选择IPD策略，群体内仍可进化出合作行为。然而，与经典双选IPD博弈相比，[24]中指出合作的进化过程更为不稳定，且更难实现[15–17]。其他研究也从不同角度印证了上述结论，例如，将线性反应策略[33]与可用ANN表示的非线性反应策略进行对比，或在特定空间结构下考察更为复杂的本地化主体交互[23]。这些发现引发了一个问题：当IPD交互涉及更多选择时，为何主体通过协同进化学习合作行为的能力会降低？值得注意的是，尽管可选行为增加，两两主体仍然在IPD对局中反复互动，如同传统双选IPD那样。

如同经典IPD那样，若主体在交互过程中以高合作水平进行互惠，群体中依然存在演化合作的机会。

因此，已有研究致力于揭示协同进化系统中影响其学习结果的特定条件。特别是，他们认为协同进化出合作主体需要满足某些条件。[24]的协同进化仿真结果显示，策略表示需具备一定的复杂性，方能实现合作行为的协同进化。而那些实现多级合作IPD策略协同进化的研究[15–17]发现，群体中策略的行为多样性，而非遗传多样性，是实现合作行为协同进化的关键。原因在于，主体策略表示参数的多样性（即遗传多样性）未必能带来个体行为多样性，而后者才反映了博弈行为的变异性。即使采用了表面上不同参数的策略，主体最终表现出的博弈行为却可能高度相似。

对策略表示条件与多选择交互下实现特定行为（如合作策略）协同进化学习的相关关注，也见于其它更复杂的交互场景。我们的研究表明，策略表示方式至关重要，因为对于某些表示形式，协同进化过程更易在主体群体内引入与维持行为多样性[12]。实际上，即使在交互受到噪声干扰的情况下，协同进化主体也可能学习到合作策略，此时高水平的合作行为可通过较低水平的合作行为来传递和表达。

但在复杂的人类交互中，合作的形成与维持还可由直接交互之外的机制所调节[29]。这也许可以解释当前仅涉及博弈直接交互的IPD协同进化学习框架所面临的局限与挑战。
间接互惠的机制被认为能够促成当前合作伙伴之间的合作，这种机制基于个体过去与他人互动时的行为信息。与直接互惠不同，直接互惠中的合作依赖个体之间的重复遭遇（例如，被建模为IPD博弈的情形），而在间接互惠中，个体由于对他人的合作行为而获得来自第三方的合作。在文献[29]中，双人互动被建模为单回合捐赠博弈。个体被随机选为施予者和受益者。被选为施予者的个体有两个选择：合作（帮助）和背叛（不帮助）。任何一对施予者-受益者的互动结果为：（1）如果施予者合作，则施予者支付代价$c$，受益者获得收益$b$，且满足$b > c$；（2）如果施予者背叛，则双方的收益均为零。

研究[29]提出了一类印象评分策略家族，并探讨了它们如何通过间接互惠促进行为合作。在该模型中，每个个体都拥有一个“声誉值”，该声誉值在捐赠博弈互动中为其自身及其合作伙伴所知。声誉的计算有两种方法：印象得分[30]和权威地位[26]。如果个体选择合作，其声誉分数将会提高；这两种方法在个体背叛时声誉分下降的方式存在差异。对于印象得分，当个体背叛时其声誉分数无论情境都会下降；而在权威地位方法中，只有当背叛缺乏正当理由（如受益者拥有良好声誉）时声誉分数才会下降。印象评分策略依据声誉分数及一些用于决策的内部参数（如阈值$h$[26]）进行行为选择。例如，一类较宽泛的策略仅依据自身声誉分数作出决策，即当声誉分数至少为$h$时选择合作。

在本章中，我们的案例研究[13]借助协同进化模拟，考察了自适应主体在包含现实系统中被认为存在的复杂互动机制下的行为，其动机非常具体。第11.3节展示了案例研究的第一部分，以更清晰地揭示在协同进化学习系统的互动中，当选择数量增加时，直接互惠机制（例如重复遭遇）在促进合作方面的有效性降低。这条发现进一步促进了案例研究的第二部分（第11.4节）：我们将探讨引入基于主体过往与其他主体互动的间接互惠机制（如声誉）[26, 30]，以建立当前与对方主体的合作。这也扩展了几乎所有采用协同进化学习的IPD博弈研究，这些研究中策略仅考虑历史行动作为当前行动选择的依据。在我们具体的案例研究中，策略在决策时现在会同时考虑前置行为和声誉两个输入因素。事实上，直接互惠与间接互惠机制在如人类社会般的复杂互动中共存，其中间接互惠机制通常是其他个体间直接互惠的结果[29]。因此，我们的协同进化模拟能够……
我们的目标是更好地揭示这些机制为何以及如何通过智能体的策略性交互促使其学习合作行为。

我们此前在[35]中已经研究了带有声誉的囚徒困境（IPD）博弈。我们的初步研究表明，即使在拥有更多选择和相应更短交互回合的IPD博弈中，通过协同进化也能更容易地学习到合作策略。在我们的实现中，每个智能体都带有一个声誉或形象分数，该分数是根据在第一阶段中从一小部分随机博弈中获得的收益计算得出的。之后，在第二阶段，智能体之间的博弈将同时考虑重复交互和声誉。这些后续博弈的结果会影响智能体的繁殖资源。在我们后续将在第11.4节中介绍的研究中，我们将用智能体实际采取的行动来代替收益来计算声誉分数。这一新实现更真实地反映了智能体的合作性。我们注意到，我们的研究与其他研究代理之间间接互惠机制的工作不同。特别是，[26, 30]中的博弈策略将声誉编码于策略的行为响应中，即决策过程中只使用声誉。此外，他们采用进化博弈论或生态方法，策略是预设的，其替换取决于种群中现有策略所占比例。

此外，我们的案例研究还将针对此前初步研究[35]中遗留的关于间接互惠机制的重要问题进行探究。目前尚不清楚声誉为何以及如何通过协同进化促进了在拥有更多选择和更短博弈回合的IPD博弈中合作策略的学习。我们的研究旨在更好地揭示间接互惠机制在协同进化智能体中促进合作行为的途径。我们还将研究不同声誉实现方式在协同进化过程中对合作策略学习的影响。我们提出了多种声誉分数的计算方法，这些方法基于其他智能体如何评估某一智能体声誉的不同解释，从而影响未来交互行为。我们关注于这些方法中声誉估计的准确性，且此准确性取决于两个因素：(1) 如何将前代智能体的博弈记忆纳入声誉分数的计算，(2) 声誉分数更新的频率。我们将评估提高声誉估计准确性是否会对协同进化中合作策略的学习产生显著且积极的影响。
11.3 通过直接互惠实现合作时多选项带来的挑战

本节展示了我们案例研究的第一部分[13]，该研究采用了多智能体共进化学习系统，以更深入地探讨为何在可供选择的游戏选项数量增加时，智能体学习高度合作策略变得更加困难。这要求智能体在交互开始时执行友好（即完全合作）的策略，并在此后持续保持相互合作。在第11.3.1节中，我们将介绍多智能体共进化学习系统的具体设计，以及我们用于运行共进化模拟的一般设置细节。随后，在11.3.2节中，我们将呈现共进化模拟的结果，并在11.3.3节中分析为什么当可用选项增加时，策略实际上拥有了更多通过采取较低合作水平而剥削他人的机会。这与涉及经典IPD（囚徒困境迭代）交互的情形不同，在经典情形中，直接互惠的机制允许智能体快速回馈合作或惩罚背叛。选项数量的增加使得策略难以判断中间选项的意图，例如，它究竟是激发进一步合作的信号，还是一种微妙的剥削。因此，当无法分辨伙伴意图时，智能体会逐步适应并实施以较低合作水平换取短期更高收益的策略。在第11.3.4节中，我们将更为详细地考察在共进化模拟中观察到的具体合作策略，并分析这些策略是如何被共进化智能体群体学习到的。
11.3.1 用于IPD的多智能体协同进化学习系统

我们用于研究 n 选囚徒困境（IPD）博弈策略学习的多智能体协同进化学习系统，基于已经在诸多竞争性博弈协同进化学习研究中应用的协同进化学习框架，例如 [7–9, 21, 22, 32]。在介绍系统设计时，我们首先讨论策略表示的选择，因为它将直接影响协同进化学习系统中其他组件（如变异算子）的设计。根据我们早期关于IPD游戏协同进化学习的研究 [12, 35]，我们采用了一个固定结构的前馈人工神经网络（ANN）。为便于与已有研究进行直接比较，我们考虑了确定性和反应型的记忆一阶策略（即基于自己与对手上轮动作做出回应）。具体来说，所用的ANN为多层感知机（MLP），包含四个输入节点、一个单隐藏层（包含十个隐藏节点）和一个输出节点。四个输入节点的输入如下 [13]：

1. 智能体上一次的选择  
2. 对手上一次的选择  
3. 若对手上一次的合作水平低于本体，则输入为 $+1$，否则为 $0$  
4. 若本体上一次的合作水平低于对手，则输入为 $+1$，否则为 $0$  

后两个输入项用于使MLP可以更便捷地比较智能体与对手的合作水平。每个节点（感知器）均关联有输入权重和一个偏置项。针对 n 选IPD 游戏，MLP 总共有 $N_w = 61$ 个连接权重。这个数目来自于 $(4 + 1) \times 10$（单隐藏层节点数）和 $(10 + 1) \times 1$（输出节点数）。激活函数采用 $\mathrm{tanh}$，其输出范围缩放到 $[-1, 1]$。之后，MLP 的输出会根据 n 选IPD 博弈的可选动作集合四舍五入到最近的动作取值。
算法 11.1 基于协同进化学习的对策演化算法（CEA）用于囚徒困境（IPD）博弈 [13]

输入：$Y$ 候选解种群  
$\lambda$ 父代种群规模  
$T$ 进化代数  
$n$ IPD博弈中的选择数

输出：$X$ 进化得到的候选解种群  

1: 过程 $\mathrm{CEA}(Y, \lambda, T)$  
2: $t := 1$  ^ 初始化时间步并开始循环  
3: $X := \mathrm{initialize}(Y)$ ^ 在 $X$ 中初始化父代种群  
4: while $t \leq T$ do  
5: \phantom{a} $X := \mathrm{variation}(X)$  ^ 对每个 $X[i], i = 1, \ldots, \lambda$ 应用变异算子，生成子代个体 $X[i], i = \lambda + 1, \ldots, 2\lambda$  
6: \phantom{a} $\mathrm{roundTour}(X, 2\lambda, n)$  ^ 在种群 $X$ 上执行循环锦标赛，获得个体 $X[i], i = 1, \ldots, 2\lambda$ 的适应度评价 $X[i].f$  
7: \phantom{a} $X := \mathrm{selection}(X)$  ^ 按照 $X[i].f$ 值降序排列，对种群 $X$ 应用选择算子  
8: \phantom{a} $t := t + 1$  
9: end while  
10: return $X$  
11: end procedure  

在确定了策略表示的具体形式后，我们采用了基于人工神经网络（ANN）实值权重变异的协同进化算法（CEA）[34]，用于自适应智能体的学习过程。此处所用的CEA基于我们早期研究的方法 [12]。一般而言，CEA是对自适应进化规划（self-adaptive $(\lambda + \lambda)$ Evolutionary Programming）[36] 的一种改进，主要区别在于CEA中候选解（智能体）的适应度评价是基于与种群中所有其他智能体的累计博弈结果（如循环锦标赛）获得的。鉴于IPD博弈的对称性（即所有智能体均可从同一策略集合中选择策略），采用了单种群的对抗性CEA。算法11.1详细说明了该CEA的过程式实现。

主过程CEA输入代理个体种群 $Y$，其中每个智能体 $i = 1, \ldots, 2\lambda$ 的策略表示（多层感知机，MLP）以向量形式 $Y[i].w$ 存储，并配备有相应的方法用于输入-输出响应的计算。由于采用了自适应变异，每个个体还包含自适应参数向量 $iY.\sigma$。每个智能体的适应度存储于 $Y[i].f$。传入CEA的其他参数还包括父代种群的规模 $\lambda = 15$ 和进化代数 $T = 600$。这些参数的设置旨在便于观察协同进化仿真过程（如合作或背叛的持续周期）。每组实验独立重复30次。

我们将在算法11.1中对其他步骤进行详细说明。初始父代个体种群由 initialize 过程生成，具体地，对于每个父代个体 $X[i], i = 1, \ldots, \lambda$ [13]：
1. 权重和偏置向量 $\boldsymbol{w} = (w_j : j = 1, \ldots, N_w)$：每个 $w_j = u \sim U[-1,1]$，即初始化时从区间 $[-1, 1]$ 上的均匀分布中随机抽取一个值赋给 $w_j$。

2. 自适应参数向量 $\boldsymbol{\sigma} = (\sigma_j : j = 1, \ldots, N_w)$：每个 $\sigma_j = 0.05$，即以固定值 $0.05$ 进行初始化。

3. 预博弈输入 $(\mathrm{pregame}_1, \mathrm{pregame}_2)$：每个 $\mathrm{pregame}_k = \mathrm{choice} \sim U\{\mathrm{choice}_1, \ldots, \mathrm{choice}_n\}$，即从 $n$ 个备选项 $\{\mathrm{choice}_1, \ldots, \mathrm{choice}_n\}$ 的离散均匀分布中随机抽取一个选项进行初始化。

在进化过程中，将变异操作应用到父代种群 $\boldsymbol{X}[i], \ i = 1, \ldots, \lambda$ 上，以产生子代种群 $\boldsymbol{X}[i], \ i = \lambda+1, \ldots, 2\lambda$。具体来说，采用自适应高斯变异算子，对多层感知机（MLP）的权重和偏置参数进行加性扰动。该算子作用于每个父代理 $\boldsymbol{X}[i]$ 的相关参数，从而生成对应的子代理 $\boldsymbol{X}[\lambda + i]$。对每个 $i = 1, \ldots, \lambda$，如下更新参数 [13]：

\[
\begin{aligned}
\boldsymbol{X}[\lambda + i].\boldsymbol{\sigma}[j] &= \boldsymbol{X}[i].\boldsymbol{\sigma}[j] \times \exp\big(\tau \times \mathbf{randNorm}(0,1)\big), \quad j = 1, \ldots, N_w, \\
\boldsymbol{X}[\lambda + i].\boldsymbol{w}[j] &= \boldsymbol{X}[i].\boldsymbol{w}[j] + \boldsymbol{X}[\lambda + i].\boldsymbol{\sigma}[j] \times \mathbf{randNorm}(0,1), \quad j = 1, \ldots, N_w,
\end{aligned}
\]
其中 $N_w = 61$, $\tau = (2(N_w)^{0.5})^{-0.5} \approx 0.2530$。函数 $\mathbf{randNorm}(0,1)$ 的每次调用均独立地从标准高斯分布 $N(0,1)$（均值为 0，标准差为 1）进行采样，用于每一次 $j=1, \ldots, N_w$ 的随机扰动。

每个预博弈输入 $\mathrm{pregame}_k$ 也会单独发生变异。其变异方式是将原值加上一个步长，该步长的概率分布由近似标准高斯分布的概率质量函数描述，因此较小的正负步长更有可能被选中。新的值不会循环（wrap around），大于 $+1$ 的值直接设为 $+1$，小于 $-1$ 的值直接设为 $-1$。

roundTour 过程将在整个种群 $\boldsymbol{X}$ 上执行循环赛，每个个体和所有其他个体（包括自身）两两对弈。每个智能体总共参与 $2\lambda$ 场对局。适应度值为所有 $2\lambda$ 场对局结果的总和，并存储于 $\boldsymbol{X}.f$ 中。

selection 过程将根据适应度值选取表现最优的 $\lambda$ 个体。具体实现是将 $\boldsymbol{X}$ 按照 $\boldsymbol{X}[i].f, \ i = 1, \ldots, 2\lambda$ 的值降序排序，使得前 $\lambda$ 个体 $\boldsymbol{X}[i], \ i = 1,\ldots,\lambda$ 作为下一代的父代种群。

11.3.2
重新审视拥有更多选项的囚徒困境（IPD）：背叛行为的演化
在此，我们展示了我们的案例研究[13]，旨在重新审视这样一个问题：当在IPD（囚徒困境）博弈交互中选择空间增大时，个体更有可能通过协同进化学习到背叛的策略。大多数以往采用协同进化模拟的研究[15–17, 24]指出，直接互惠机制在促进合作性协同进化学习方面的有效性较低。我们从两组受控实验展开协同进化模拟。第一组实验采用64选项的IPD，第二组实验则采用4选项的IPD。两组实验中IPD博弈的局数均固定为十轮。这意味着，在第一组实验中，选项个数（如64）大于博弈进行的轮数（10）；而在第二组实验中，正好相反（如4个选项对应10轮）。通过这些实验，我们可以研究在选项数显著多于轮数的IPD游戏中，直接互惠机制在协同进化学习中的有效性是否与选项数较少的情境一样。需要指出的是，为便于分析，我们将运行结果分类：(1) 若最终种群平均收益不超过$1.5$，则判定为“相互背叛”结果；(2) 若最终平均收益不少于$3.5$，判定为“相互合作”结果；(3) 若最终平均收益在$1.5$到$3.5$之间，则判定为“中等合作”结果。以此方式，我们在后续图表图例中分别用“Defection”（背叛）、“Intermediate”（中间状态）和“Cooperation”（合作）来表示相互背叛、中间状态和相互合作的运行次数。在进行统计检验进行比较时，我们采用成对Wilcoxon符号秩检验，显著性水平为$0.05$，随后报告各实验组的均值和标准差。图11.3总结了两组实验的结果。我们的结果证实了以往研究[15–17, 24]中的观察：当选项数高于重复轮数时（如64选项的IPD），种群更易协同进化至相互背叛的行为。具体而言，图11.3表明，在64选项IPD设置中，30次实验中有25次出现了相互背叛，而在4选项的设置下，仅有9次出现了相互背叛。通过比较两组实验各自独立运行的种群平均收益，可以观察到两组实验结果之间存在统计显著差异（4选项IPD的$2.27 \pm 1.13$，64选项IPD的$1.37 \pm 0.84$）[13]。接下来，我们对协同进化个体的实际博弈行为进行更深入的分析，统计最优（即适应度最高）个体在所有独立运行实验中的行为反馈。我们的结果显示，在协同进化代理个体的实际动作实现中，．．．

图11.3 不同选项数（4选项与64选项）的IPD协同进化实验结果对比。结果为协同进化学习过程结束时的统计。
在64选项IPD实验设置中，相比于4选项IPD，更“具有攻击性”的策略（例如在游戏开始时采用较低合作水平）出现的次数更多（即26次对19次）。此外，在拥有更多选择（即64选项）的IPD游戏设置下，如果前几步表现为互惠合作，协同进化策略以较低合作水平进行响应的运行次数也更多（例如24次对13次）。这表明，在种群中实现高度合作性的博弈更为困难，因为协同进化的智能体不愿意持续进行互惠合作。然而，结果也表明，这些智能体并非在所有运行中都表现得很“天真”（例如在前几步表现为互惠背叛时，能够以较高合作水平作出回应），这在两组实验中都是成立的。事实上，它们实施的策略在前一轮发生互惠背叛时，会做出偏向背叛的选择 [13]。我们通过单独检查最终协同进化种群中的策略输入-输出反应，对这些智能体进行了细致的观察。在大多数64选项IPD实验中，种群进化到偏向背叛的情况下，我们发现智能体采用的策略多是在开局时采用低合作水平，并在后续一直以低合作水平响应。此外，种群中的大多数智能体也采取了相似的策略 [13]。我们的协同进化模拟表明，当选择数量增加时，直接互惠机制在促进合作行为学习方面的效果会变差。这一现象在选择数目超过博弈轮数时更加明显。因此，我们又开展了两个更为严格对照的实验以进一步阐明这一观点。我们将等式(11.1)从
$$
p_A = 2.5 - 0.5c_A + 2c_B
$$
修改为
$$
p_A = 2.5 - 0.25c_A + 2.25c_B
$$
这样做是为了减少在IPD收益函数中对高合作水平的惩罚，进而应该能够促进协同进化过程中合作行为的学习。需要注意的是，对于经典IPD而言，图11.4展示了对比实验结果：在使用较低合作惩罚的收益设置下，4选项和64选项IPD两组实验在协同进化学习结束时的结果对比。
其得到的收益为$(R, S, T, P) = (4.5, 0.5, 5, 1)$。图11.4总结了这两组实验的结果。对于这些新的实验设置，即分别对应有64种选择和4种选择的IPD博弈，更容易获得合作的结果。然而，总体来看，绝大多数共进化模拟的运行结果表明，种群会朝向低合作和倾向背叛的博弈方式共进化【13】。

11.3.3 更多选择为剥削合作伙伴创造了更多机会

我们的目的是进一步探究为什么更多的选择会导致代理在共进化学习中倾向于采纳背叛策略。我们统计了在包含64种选择和4种选择的IPD游戏的共进化模拟中，代理实现的策略对可用选择的采样情况。首先，我们在每个代理所实现策略的行为响应映射中，计算其采样响应分布的熵。对于$n$选择的IPD游戏，记忆一（memory-one）反应型策略的行为响应映射包含$n_t = n^2$个元素，这些元素用于考虑在上一回合中自己和对手的联合出招。熵的计算方式如下【13】：

$$
- \sum_{l}^{n_t} \left( \frac{m_l}{M} \cdot \mathbf{log}_{2} \frac{m_l}{M} \right),
$$

其中，$\frac{m_l}{M}$表示策略映射中第$l$个元素被采样的频率，$M = \sum_{l}^{n_t} m_l$为总采样次数。公式（11.2）应用于每一代中的最优代理，以监控其策略在共进化学习过程中对博弈的响应行为。我们采用未归一化的熵值，因为我们的分析着重于被采样响应总数的多少，即有多少备选选择被实际采用。归一化熵则表示实际选用的可用选项所占比例。
对IPD博弈中带有4个和64个选择的协同进化学习过程，作者对30次运行中600代的平均熵值进行了绘制 [13]。熵值的计算通过公式(11.2)来实现，它为代理如何分配交互机会以采用不同选择提供了量化度量。较高的熵值说明代理在面对以往行动组合时选择了更多的替代响应，因此策略在实际执行时对不同选择的采样更加广泛。我们的结果显示，30次运行取最后一代时的平均熵值，对于4选择的IPD博弈为$0.68$，而对于64选择的IPD博弈为$1.34$。这通常表明，相较于4选择的情境，64选择的IPD博弈中代理所采取的选择更为多样 [13]。

我们注意到，在64选择的IPD实验组中，平均熵值下降非常迅速。对单独运行的进一步考察显示，种群在从初始随机策略进化过程中，更频繁地演化为低水平合作的策略。在协同进化早期，从初始时对各选择采样频率分布较为均匀，到后期分布变窄的变化，可在图11.5中观察到，该图展示了4次样本运行中分布的变化情况。虽然采样的选择数量增多，但这些选择大多集中在接近背叛的低合作水平。与此相反，在包含4个选择的IPD协同进化模拟中，有部分运行显示出种群能够演化至采用中等水平选择和实现互惠合作。

通常情况下，人们会预期，增加可选策略会为促进合作与利用伙伴双方创造更多机会。尽管在短期内（如当前轮次）利用合作伙伴的收益高于实现互惠合作，但代理仍有可能通过反复交互学习实施策略，通过直接互惠机制回报合作、惩罚背叛，从而推动互惠合作的实现。然而，我们对协同进化模拟的更深入分析表明，实际情况并非如此。当可选择的策略数量增多时，通过协同进化学习过程实现互相背叛的结果始终比实现互惠合作的结果更为普遍。

需要注意的是，协同进化学习过程由代理在所有已参与IPD游戏所获得的累积收益驱动。因此，协同进化模拟的结果表明，代理更有动力调整策略以利用其伙伴，而非促进合作。换句话说，当可选策略变多时，在交互过程中有效的可利用伙伴的机会也随之增多。这削弱了直接互惠机制在促进协同进化学习合作行为方面的效果 [13]。
图 11.5 展示了64选择IPD实验中各个种群选择的采样分布。选择从0（完全背叛）到63（完全合作）表示。对于某一特定选择和某一特定世代，更高的纵向线条表示该选择的采样频率更高 [13]。

造成直接互惠机制在协同进化学习中效果下降的一个解释是，在可用选择数量增多的互动环境下，策略难以对中间选择做出有效判别——无论是将其作为促进进一步合作的信号，还是作为一种微妙的剥削。尽管有重复的交互，但选择数的适度增加会导致个体可行选择序列数目大幅增加，这使得通过博弈结果（累计收益）来判断中间选择意图变得更加困难。因此，我们通过增加64选择IPD的博弈时长，进行了额外的实验以验证这一解释。

如图11.6所示，我们的协同进化模拟结果表明：随着博弈时长的增加，个体学习低水平合作策略的倾向有所降低。在将博弈时长增加至30后，进化至完全背叛的实验次数由25次降低到15次（注：当博弈时长延长至150轮时，未观察到进一步下降）。具体而言，30轮和150轮实验的平均收益和标准差分别为 $1.89 \pm 0.94$ 和 $1.80 \pm 0.91$（见图11.6）。图 11.6 对比了使用较长博弈时长下64选择IPD实验的结果。结果取自协同进化学习运行结束时。
分别得到了这些结果。将其与在博弈轮数为10的情况下获得的结果（$1.37 \pm 0.84$）进行比较[13]。然而，群体中互相背叛的结果的减少被取而代之的是那些选择中等程度合作的个体。这一现象通过对个体如何采样备选行动的分析进一步得到了验证，即在博弈持续时间更长的协同进化群体中，平均熵度量更高。具体来说，在最终一代的30次实验中，64选项IPD博弈，博弈轮数分别为10、20、30和150时，对应的平均熵分别为$1.34$、$1.96$、$2.16$和$2.02$[13]。

11.3.4 合作性博弈仍然可能吗？

在这一点上，也许有人会问，通过直接互惠机制，个体是否还可能学习到高度合作的行为？与经典IPD博弈环境中具有使个体群体从初始背叛学习实现互惠合作能力的以“以牙还牙”为代表的策略家族不同，当互动环境包含更多可选项时，需要采用其他策略。在这里，我们考虑“逐步加码”（Raise-the-Stakes）策略家族[31]，该类策略能够在更多可选策略的互动中利用直接互惠机制。这类策略会在与伙伴的互动过程中逐步提升合作水平，以此鼓励更高程度的合作，直至实现互惠合作。这些策略将面临的挑战在于如何平衡合作水平提升到足以迅速达成互惠合作时，不至于在对方中途停止提升时反而被其剥削[31]。
我们研究了在一次进化过程中，种群进化到相互合作博弈时，个体所采用的策略。共进化结束时，种群中表现最好的个体所采用的策略具有一定形式的“加码策略（Raise-the-Stakes）”特征。种群中大多数个体也采取了类似的策略。这一策略表征的特点是在经历了初始的完全背叛（full defection）之后，策略表现出一个陡然跃升至完全合作（full cooperation）反应的宽广区域，这表明该策略对对方采取其他选择表现出一定的容忍性，并以完全合作作出回应。尽管发现了这一现象，但进一步分析个体在整个共进化学习过程中的反应发现，合作性行为并不是由“加码策略”产生的。换句话说，“加码策略”并不是促进共进化学习过程在原本进行相互背叛的种群中进化出合作策略的垫脚石[13]。在策略提升合作水平的速度与避免自身易受剥削之间，必须保持微妙的平衡[31]。

图11.7展示了共进化个体所采用策略最具代表性的三个属性（例如，第一次行动、对相互合作的响应以及对相互背叛的响应）。在第256代左右，策略开始以较友善的方式进行互动（即，具有非常高的合作水平），种群合作水平从较低跃升到更高。完全合作的响应，即对前一次相互合作博弈做出完全合作，在短短九代之后就得以学会（见图11.7中间图）。这些策略更接近于“以牙还牙（Tit-for-Tat）”。大约再经过60代，我们观察到上文已讨论的类似“加码策略”在种群中出现。类似地，从低水平合作到高水平合作的快速跃迁，也可以在共进化种群的行动选择频率采样分布图中观察到[13]。

我们的共进化仿真结果显示，合作行为是在原本以相互背叛为主的种群中，通过个体在互动之初就采纳高度合作化的策略而涌现的。其他研究（如[33]）也报告了类似的观察结果，并提出，在互动伊始便采取高水平合作非常重要，因为这避免了不得不通过加码逐步建立信任的过程。
图11.7 演化过程中最佳个体所实现策略的特性属性。顶端的图展示了首次行动时的选择。中间的图展示了如果前几次行动是双方都完全合作（即策略和对手均为完全合作策略）时的选择。底部的图则展示了如果前几次行动是双方都完全背叛（即策略和对手均为完全背叛策略）时的选择 [13]。

11.4 在协同进化中引入间接互惠机制

在本案例研究的第二部分 [13]，我们的目的是演示为何以及如何在IPD（囚徒困境博弈）游戏中引入声誉机制可以促进合作行为的协同进化学习。我们的协同进化模拟结果将揭示，智能体会学习出利用声誉机制来估计未来合作伙伴行为的策略，并且在互动伊始就能够激励对方合作。首先，在第11.4.1节中，我们将介绍在协同进化智能体的策略决策过程中具体实现声誉机制的方法，这些智能体还通过IPD游戏进行重复互动。在第11.4.2节中，我们将展示协同进化模拟的结果，表明通过引入声誉，群体中可以实现互惠合作的结果。接下来，在第11.4.3节中，我们将分析，为何以及如何智能体群体中能够出现互惠合作行为。特别地，我们观察到，尽管存在其他中间性选择的可能，智能体实际上实施的策略有效地表现为二元选择（例如完全合作和完全背叛）。当学会执行此类策略的智能体协同进化并始终维持高声誉分数时，就能够获得合作结果。这主要得益于选择性博弈，即：对于拥有良好声誉（例如声誉分数相等或更高）的对手完全合作，否则对于声誉较差（例如声誉分数较低）的对手则完全背叛。最后，在第11.4.4节，我们将介绍进一步的协同进化模拟，以验证对群体而言，在声誉评估阶段通过合作来维持高声誉分数是有益的。
11.4.1 声誉的实现

与我们在前期研究 [35] 中采用的方法类似，本文采用基于二元比较的声誉实现方式（即好与坏）。具体而言，个体的策略决策依据于声誉分数的比较：如果伙伴拥有不低于自身的声誉分数，则该个体认为对方声誉良好；否则，则认为对方声誉较差。通过声誉分数的比较（即基于相对值而非绝对值）来实现声誉，动机在于前人工作 [26, 30] 的启发。在此，通过声誉分数的比较，为个体高效且明确地识别那些此前在高度合作中承担过成本（例如曾面临被利用风险）从而获得高声誉分数的潜在合作伙伴，提供了便利。如果使用绝对值实现，则识别难度将更大。至关重要的是，我们的实现同时关联了直接互惠和间接互惠两类机制。由于个体在决策时会同时考虑以往的行为和声誉分数，因此所有可能参与互动的个体，其声誉分数须事先确定。我们采用了之前在 [35] 中提出的针对多选与声誉的囚徒困境（IPD）两阶段过程。首先，在第一阶段（声誉估算阶段），个体基于与当前世代种群中其他个体在若干随机的$n$选IPD博弈中的选择行为来计算其声誉分数。之后在第二阶段，个体通过带有声誉信息的$n$选IPD进行互动。每个个体都能从两个阶段的互动中获得$n$选IPD的博弈收益。因此，第一阶段的随机抽样为不放回抽样。需指出，这种双阶段的声誉估算过程是较为简化的实现，后文将进一步讨论更具现实性的过程及其对个体策略行为协同进化学习的影响。基于随机抽样自种群的个体，通过IPD游戏估算单个体声誉分数的过程描述如下。尽管某些个体在第一阶段参与的游戏数量可能多于其他个体，本流程通过对所有参与游戏的选择比例进行归一化。记$\boldsymbol{p} = (p_1, \ldots, p_n)$为某个体在所有拟用来估算声誉的游戏中的经验离散分布，其中$p_i$表示该个体选择IPD $n$选中的第$i$项的比例。记$\boldsymbol{x} = (x_1, \ldots, x_n)$表示$n$个选项的权重。具体地，我们采取$(x_1, \ldots, x_n) = (\mathrm{choice}_1, \ldots, \mathrm{choice}_n)$，即完全背叛的权重$x_1 = \mathrm{choice}_1 = -1$，而完全合作的权重$x_n = \mathrm{choice}_n = 1$。由此，声誉分数的计算方式如下 [13]：
$R = \sum_{i = 1}^{n} x_i \cdot p_i \\ = \frac{\text{所有选择之和}}{\text{总步数}}。 \nonumber$

请注意，个体的声誉分数是基于其在第一阶段中的实际选择计算的，而不是之前在 [35] 中所采用的收益。在这种方法下，声誉分数能够更准确地反映个体的实际合作性。将收益作为合作性指标容易产生误导，例如，更高的平均收益既可能来源于双方互惠合作，也可能源于个体对其伙伴的剥削。

实现具备更多选择的IPD（囚徒困境）博弈共进化学习过程的算法11.1可以很容易地适配至引入声誉机制的交互设定。首先，个体的数据结构现在为：

${\boldsymbol X}[i] = \big({\boldsymbol w},{\boldsymbol \sigma},(\mathrm{pregame}_1,\mathrm{pregame}_1),{\boldsymbol p},{\boldsymbol x},R,f\big)_i$

需要注意的是，用于具备声誉机制的$n$选项IPD博弈策略表示的MLP（多层感知机）需要增加一个输入节点，该节点的取值如下：若对手的声誉分数等于或高于本体的，则输入$+1$，否则输入$-1$。在声誉评估阶段，第五个输入节点的取值为0。综上，网络参数数目为：

$N_w = (5 + 1)10 + (10 + 1)1 = 71$

因此参数为 ${\boldsymbol w} = (w_j : j = 1,\ldots,N_w)$，${\boldsymbol \sigma} = (\sigma_j : j = 1,\ldots,N_w)$，以及自适应参数：

$\tau = (2(N_w)^{0.5})^{-0.5} \approx 0.2436$

所有相关的过程都被更新以保证和声誉有关的数据能够被妥善处理，例如初始化等。另一个主要的更新涉及原始的 roundTour 过程，此过程现在须整合两阶段对弈过程，现由 ROUNDTOURREP 实现，对应的步骤描述见算法11.2。

这里所描述的过程利用了博弈的对称性，因此共有$(\lambda^2 + \lambda) / 2$ 个独特的配对交互。被选为声誉评估的个体对所参与博弈的概率由参数$rpr\_p$设定，最大不超过$0.5$。应注意，策略选择是基于两个阶段（声誉评估阶段与具备声誉的$n$选项IPD阶段）游戏的综合收益。该过程实现了间接互惠的机制——即个体当前配对互动是由其与他人历史直接互惠交互所决定的 [29]。

算法11.2 两阶段博弈的循环赛（Round-robin tournament with two-stage game）[13]
输入：$\mathbf{X}$ 候选解的完整种群  
$2\lambda$ 完整种群的规模  
$n$ IPD博弈中的选项数量  
$r_p$ 被选为声誉评估的配对交互概率  
输出：$\mathbf{X}$ 带有更新适应度值的完整种群  

1: 程序 ROUNDTOURREP($\mathbf{X}$，$2\lambda$，$n$，$r_p$)  
2: $\mathbf{X} := \mathrm{initializeRep}(\mathbf{X})$ // 初始化声誉分数 $X[i].R = 0,\ i = 1,\ldots,2\lambda$  
3: 对 $i = 1 : 2\lambda$ 执行 // 阶段1：声誉估计博弈  
4: 对于 $k = i : 2\lambda$ 执行  
5:  如果 $\mathrm{rand}(0, 1) < r_p$ // 选择该配对进行声誉评估  
6:   $\mathrm{selStage}[i][k] := \mathrm{true}$  
7:   $\mathrm{IPDREPEST}(X[i], X[k], n)$ // 更新该对智能体的声誉分数和适应度值 $X[i].R, X[i].f, X[k].R, X[k].f$  
8:  否则  
9:   $\mathrm{selStage}[i][k] := \mathrm{false}$  
10:  结束条件  
11: 结束循环  
12: 结束循环  
13: 对 $i = 1 : 2\lambda$ 执行 // 阶段2：带声誉的IPD博弈  
14: 对于 $k = i : 2\lambda$ 执行  
15:  如果 $\neg(\mathrm{selStage}[i][k])$ // 未选择该配对用于声誉估计  
16:   $\mathrm{IPDREP}(X[i], X[k], n)$ // 更新该对智能体的适应度值 $X[i].f, X[k].f$  
17:  结束条件  
18: 结束循环  
19: 结束循环  
20: 返回 $\mathbf{X}$  
21: 程序结束  

11.4.2 声誉促进合作行为  

首先，我们证实在具有更多选项甚至是短时长的IPD博弈环境中，声誉机制能够促进合作行为的协同进化学习。为此，我们考虑了一个包含64个选项且博弈时长为十轮的IPD交互设置。对于进一步涉及声誉因素的交互设置，我们考察了如下取值的结果：$r_p \in \{0.05, 0.15, 0.25, 0.50\}$。图11.8总结了带声誉与不带声誉实验的协同进化仿真结果对比。总体而言，我们的结果表明，将声誉机制引入IPD交互能够促进智能体的合作行为的协同进化学习。特别地，即使在选项数为64且博弈轮次极短（仅十轮）的IPD情形下，种群也能学会相互合作，而在这类环境中，协同进化的种群本来很难建立起相互合作关系（参见第11.3.2节）。我们的结果还揭示了设定$r_p$时在协同进化过程中的权衡关系。图11.8展示了在有无声誉机制下，64选项IPD博弈实验结果的对比。所有结果均是在协同进化学习过程结束时统计得到。
结果显示，随着 $r_p$ 从 $0.05$ 初步增加到 $0.15$，合作结果增多，但当 $r_p$ 增至 $0.50$ 时，合作行为又减少。统计分析表明，当 $r_p$ 设置为 $0.15$ 和 $0.25$ 时，对应的实验组结果与没有声誉机制的基线实验组存在显著性差异。具体来说，$r_p = 0.15$ 时的结果为 $2.93 \pm 1.40$，$r_p = 0.25$ 时为 $2.60 \pm 1.51$，而无声誉机制的基线为 $1.37 \pm 0.84$。鉴于两个阶段博弈的收益均被用于个体适应度的评估，这种权衡可能反映了以下情况：既要保证有足够的博弈次数来可靠地估计个体声誉分数，又要使得具有声誉机制的博弈次数足够多，从而个体可以利用声誉达成互惠合作，在生殖适应度方面获得有利的结果[13]。

11.4.3 声誉为何以及如何促进合作

在本节中，我们进一步探讨协同进化学习过程，以便更好地理解声誉机制为何以及如何促进个体群体参与互惠合作行为。我们首先分析个体行为映射中的博弈采样分布，并比较有声誉机制和无声誉机制下，囚徒困境（IPD）博弈交互设定的实验结果。当声誉参数 $r_p=0.05$ 以及无声誉机制的实验组中，群体协同进化过程中的平均熵值分别下降至 $1.34$ 和 $0.40$。使用其他 $r_p$ 设置时也观察到类似趋势。我们的协同进化仿真结果表明，个体在采样行为选择时，其可选行为数更少。
图11.9 展示了在典型收敛于合作的演化过程中，IPD（囚徒困境）中具有64种选择和声誉机制的情境下，种群个体选择分布的采样频率。选择的范围从$0$（完全背叛）到$63$（完全合作）表示。对于某一代的某一选择而言，垂直线越高，表示该选择的采样频率越高。图中绘制了第$1$代到第$50$代的采样分布，用以展示从主要为多样选择博弈向二元选择博弈的转变过程[13]。当交互环境进一步引入声誉机制时，行为映射中的少部分行动模式会被采用。对单次实验的进一步分析显示，个体逐步学会以二元决策方式进行博弈，即完全合作或完全背叛。图11.9描绘了在典型实验中群体个体实际选择的分布。从图中可以观察到，在不到$100$代内，种群逐步演化为主要采用二元选择的策略。完全合作行为的采样频率高于完全背叛，表明该种群最终演化出了合作性结果[13]。由此可以确定，在共进化种群中，声誉机制促使了合作性行为的出现，其原因在于个体学会只采用完全合作与完全背叛两种二元选择策略。

接下来，作者对影响个体行为变化、推动二元选择博弈出现的演化学习过程进行了分析。具体观测了共进化种群中最优个体的三个属性：（1）第一次行动；（2）针对前一轮为双方合作的响应方式；（3）针对前一轮为双方背叛的响应方式。上述观测针对的是在声誉估计阶段以及第二阶段（即与声誉良好或恶劣的伙伴进行互动）中个体的表现。

对那些最终实现双方合作的运行结果进行分析后发现，个体在声誉估计阶段趋于天真且高度合作。这一典型演化过程表现为：无论前一轮双方采取何种联合行动（无论是双方合作还是双方背叛），个体均学会做出友好反应，即完全合作。而在进入基于声誉的第二阶段后，这些个体针对声誉良好的伙伴，演化出高度合作的博弈策略。然而，针对声誉较差的伙伴，这些个体则表现出相反的行为。进一步分析表明，这类个体进化为“不宽容型”，即对声誉较差者采取背叛策略：从完全背叛开始，并在后续互动中，无论双方先前行为如何（是双方合作还是双方背叛），都持续选择完全背叛[13]。
最后，我们对这些智能体所学习的策略进行了深入分析，以更好地理解声誉是如何促进互惠合作行为的。我们发现，共演化得到的智能体实现了一种相对简单的策略：（阶段1）其表现为“友好”——以较高的合作水平开局，并持续保持高度合作；（阶段2）则根据合作者的声誉进行区别对待的策略，即：当合作对象声誉良好时则完全合作，否则对声誉差的对象则选择完全背叛。换言之，这些策略在做出行为反应时仅考虑声誉，而之前的联合动作实际上是多余的。此外，我们注意到，这样的策略被大多数个体所采纳。这种策略能够在整个种群中成功扩散，原因在于当遇到采用类似策略的合作者时，可以在整个博弈过程中实现完全的互惠合作，从而使智能体获得最大的总收益。当在包含更多选择与声誉机制的互动场景下遭遇不合作者时，该策略也能迅速进行惩罚。对于这类交互情形，声誉实际上作为一种信号机制，引导智能体进行互惠合作。高声誉得分反映了某一智能体在未来交互中参与高度合作博弈的意愿，而声誉低者则相反。通过比较声誉得分，采用区别对待策略的智能体可以提前分辨值得信任以进行互惠合作的未来合作者，以及那些应该受到坚决背叛惩罚的对象。对多层感知机（MLP）输入-输出响应的细致分析显示，第五项输入（即声誉）对应的连接权重远高于前四个输入（即历史动作）。这种MLP权重结构使智能体能够仅基于声誉得分的比较来进行区别对待的行为。接下来，我们检视来自不同实验组生成的MLP。具体而言，在共演化学习过程中，当 $r_p < 0.5$ 时，会观察到上述权重结构的MLP。可以认为，在共演化条件下，与区别对待行为相关的性状（如第五输入：声誉）受到更高的选择压力。这种声誉输入上的强选择压力可以解释为：对于 $r_p < 0.5$，智能体更频繁地进行利用声誉（第二阶段）的博弈，且这部分对其适应度的贡献大于未使用声誉（第一阶段）的博弈。因此，在 $r_p = 0.5$ 的共演化学习过程中，我们并未观察到类似区别对待的行为。由于此时两个阶段的博弈次数平均而言相近，因此不存在足够的选择压力作用于MLP的第五输入，使其能够通过共演化学习区别对待策略 [13]。

11.4.4
保持良好声誉值得吗？
在这一点上，人们会立刻提出一个问题：共演化的智能体是否有必要维持良好的声誉。这个问题的答案非常重要，因为引入声誉作为额外的输入，会在当前互动中为了获得即时有利收益的响应与为未来互动积累潜在有利收益的响应之间构建一种微妙且复杂的关系。这两组响应通过一个机制联系起来：即当前的选择决定了声誉分数的计算方式，而声誉分数的比较又成为未来策略决策时至关重要的参考依据。请回忆，如果一个智能体的合作者的声誉分数大于等于自身的声誉分数，则该智能体会将其合作者视为具有良好声誉。我们的共演化模拟表明，在所有能够演化出合作策略的实验中，最佳智能体均获得了较高的声誉分数。对一次典型的合作结果实验过程的观察显示，成功智能体的声誉分数与种群中的合作水平状态（以平均收益衡量）是相吻合的。而在产生背叛行为的实验中，种群中最佳智能体的声誉分数会在 $-1$（完全背叛时）到 $1$ 之间波动，若未参与声誉评估阶段则为 $0$【13】。进一步细致分析发现，种群中存在声誉分数为 $1$ 的智能体，这与最佳智能体的声誉分数一致。只有在声誉评估阶段智能体执行完全合作行为时，才能获得如此高的声誉分数 $1$（参见公式(11.3)）。我们发现这些智能体采用的策略具有较大的响应域，即在声誉评估阶段的博弈中展示全合作的行为模式。在包含声誉的第二阶段互动中，这些智能体还能够从同样具有高声誉分数，并采取相似策略的其他智能体那里获得互惠合作。与此同时，它们会对声誉较低的智能体进行歧视性对待，即采取背叛策略。通过这种方式，声誉有助于促进合作，因为它能够抑制智能体选择较低合作水平的其他选择【13】。这一点在选择空间更大、持续轮次更短的场景中尤为重要，而这些场景在第11.3.3节中已被论证为共演化自适应智能体学习合作策略时的挑战性情形。不像以往只关注间接互惠机制的研究【26, 30】，我们所采用的方法面临的一个担忧是可能会出现“作弊者”：这些智能体通过获取高声誉分数被视为拥有良好声誉，从而专门利用那些与高声誉智能体合作的个体。然而，我们的实现机制抑制了智能体学习此类作弊行为。主要原因有两点，其一与变异算子有关，其二与选择算子有关。首先，需要指出的是，对于基于MLP的智能体而言，在继承了声誉评估阶段的合作行为和第二阶段的歧视性行为后，功能上很难通过重构神经网络来学习这种作弊型行为。尤其是自适应变异的应用意味着在共演化学习过程的这一阶段，智能体种群的 $\sigma$ 参数值已经非常小。第二，我们观察到这种“作弊者”智能体在与其他作弊者（包括自对弈）互动时表现并不理想。此外，共演化学习中也有可能涌现出更有针对性的歧视性策略，即在第二阶段不再只是单纯的天真合作者，而是采用以牙还牙（Tit-for-Tat）等在面对背叛策略时表现更为稳健的对策。
11.5 讨论与结论

在本章中，我们介绍了一个案例研究[13]，该研究将竞争共进化的原理应用于开发多智能体共进化系统，并通过详细的仿真分析，在复杂环境下交互时，某些条件为何以及如何促进特定行为的学习。案例研究的第一部分，旨在揭示当智能体在囚徒困境(IPD)交互中有更多可选项时，为何其共进化种群更难学会合作行为。我们的共进化仿真显示，智能体在可选择较低合作水平时，有更多机会对他人进行剥削，因为它们较难通过中间选择判断对方的意图——这种选择既可能是促使进一步合作的信号，也可能是隐晦的剥削。因此，智能体会自适应地选择背叛，以在短期内获得更高的收益。总而言之，已被提出用于解释智能体在多次交互中实现互惠合作的直接互惠机制，在这种情况下变得不那么有效。

在双人IPD博弈中，选择数量的适度增加，会导致智能体可能的行动序列数量大幅增加。基于累计IPD收益的游戏结果，智能体更难根据中间选择去解析对方的真实意图。然而，其他机制——如基于间接互惠的机制——已被证实在复杂人类互动中能够有效促进合作行为。间接互惠机制可以通过声誉(reputation)来解释，即两个个体之间的合作行为依赖于他们与其他个体的过往互动。这也促成了我们案例研究的第二部分，即在多智能体共进化学习系统的IPD交互中引入声誉机制。这与以往研究中有关间接互惠源于在复杂真实世界互动中其他个体之间产生的直接互惠的观点一致。因此，针对有多个选择及声誉机制的IPD博弈，智能体共进化学习将这两种机制有机地联系了起来。

具体而言，智能体的声誉分值由其在种群内不涉及声誉机制的少量交互中做出的选择来估算，而这些分值随后在涉及声誉的后续交互中被其他智能体使用。我们的共进化仿真显示，将声誉机制融入IPD交互，促进了共进化种群中合作行为的学习。智能体在声誉评估阶段采用合作策略以获得较高的声誉分数，而在第二阶段则采取有差别的策略——对声誉好者合作，对声誉差者背叛。
需要注意的是，我们所采用的简单两阶段方法，即在代理之间的IPD（囚徒困境）交互中引入声誉，可能并不现实，这是由于我们对声誉如何评估做出的两个主要假设。首先，在代理与种群中其他成员交互的一段时间内，代理的声誉是静态的，只有到下一次更新才会发生变化，而在当前的实现设置中，这只在下一代才会进行。其次，代理的声誉分数在每一代中都会被重新计算，因为以前的博弈并不会被记录下来。在结束本章前，我们研究了两种替代的声誉实现方式，旨在解决我们在前文提出的这两个问题。总体来说，我们从声誉估计准确性的角度看待这两种替代实现，它们都是基于代理与其他代理的过往交互。而具体而言，这两种替代实现用单阶段过程替代了原有的两阶段过程，使得任意两个代理间的每一次交互都同时包含IPD博弈和声誉的影响。需要注意的是，现在MLP的第五个输入仅取$+1$或$-1$。接下来，我们允许代理存储其在前几代中所做的选择，这些选择将被用于计算声誉分数。这一实现有效地引入了对于以往交互（包括发生在历代协同进化学习过程中的交互）中所进行博弈的记忆。因此，我们可以考察声誉的准确性如何反映代理对其未来合作伙伴行为的关联性，这与之前博弈记忆的多少（如跨代记忆）以及声誉分数更新的频率相关[13]。

我们首先讨论在第一种替代性声誉实现下协同进化仿真的结果。需要注意的是，每个代理的经验性离散选择分布$\boldsymbol{p} = (p_1,\ldots,p_n)$初始值设为$(0,\ldots,0)$，且可以被子代理继承。每个代理的声誉分数，在每一代开始时根据公式（11.3）计算，并在同一代内保持不变。在协同进化开始时，代理的声誉分数被任意地从均匀分布$U[-1,1]$中分配，这确保了对于拥有好声誉和坏声誉的伙伴，所实现策略的行为反应都能被平均采样到。我们的协同进化仿真结果表明，当协同进化的代理种群以这种方式进行拥有更多选择及声誉机制的IPD交互时，更容易出现合作结果。此外，我们还进行了额外实验，其中参数$p_u$用于设定某一博弈未被用于更新代理的$\boldsymbol{p}$的概率。结果显示，这会使协同进化向合作演化的结果数量略有下降[13]。

进一步仔细分析协同进化代理所实现的策略后发现，它们主要以二元选择，即完全合作或完全背叛作为响应。具体而言，这些代理对于拥有好声誉的伙伴表现出高度的合作性。对于拥有坏声誉的伙伴，它们也有可能合作，但如果对方的行为接近背叛，则会以背叛进行惩罚。就这一点而言，采用单阶段声誉实现的协同进化模拟中，代理所实现的策略与采用两阶段方式时观察到的有所不同。我们此前的研究已表明，基于两阶段博弈的声誉实现会因选择压力作用于声誉输入、尤其在$r_p$较低的设置下（第二阶段涉及更多带有声誉的IPD交互），而促成差别对待的策略。这一点也通过对MLP的进一步检验得到证实——与声誉输入相关的连接权重实际并不显著增大[13]。
接下来，我们讨论在合成第二种声誉实现方式下进行的协同进化模拟结果。与第一种替代实现相比，主要区别在于声誉分数在每局游戏结束后都会更新。结果表明，这种机制对合作结果产生了积极影响。总体而言，由协同进化学习过程产生的合作结果在我们所研究的不同$p_{up}$设置下都是稳定或具有鲁棒性的。关于智能体行为反应的协同进化，我们观察到与第一种实现方式下学习到的策略相似。也就是说，智能体通常学会了实施几乎完全二元化的策略——要么完全合作，要么完全背叛。对于在先前回合中表现出合作行为的良好声誉和不良声誉的伙伴，这些智能体都表现出高度的合作性。尽管这些智能体并不基于声誉本身进行歧视，但它们对不良声誉伙伴的容忍度较低（例如，对于合作水平较低的伙伴选择完全背叛）相比于对良好声誉伙伴。因此，这些策略并非完全依赖声誉来作出区分。对与这些策略相关的多层感知机（MLP）权重进行详细分析后，未发现前一回合行为与声誉输入之间的连接权重存在显著区别。这些策略采用了一种更为细微的区分方式，即在决策时同时考虑了先前行为和声誉，以及互动过程的演变方式[13]。

总之，我们的协同进化模拟指出，准确的声誉估计对于协同进化智能体群体进一步学习合作策略具有重要意义。影响这种协同进化学习结果的因素包括：用于声誉估计的先前互动（包括跨代的历史）记忆，以及声誉分数的更新频率。

在章节结束之前，我们指出，在复杂的现实世界互动中，间接互惠机制可能会受到其他效应的影响。我们已研究了一种效应，即并非所有的历史游戏都会影响智能体的声誉分数。然而，互动中的其他扰动（如噪声）也可能影响适应性智能体的协同进化学习过程。需要进一步研究和确定这些互动中具体类型的噪声（例如，涉及认知上的改变还是对实际行为的改变）及其水平，以评估它们对合作行为协同进化学习所带来的重大挑战。
参考文献

1. Axelrod, R.：囚徒困境中的有效选择。《冲突解决杂志》24(1)，3–25 (1980)
2. Axelrod, R.：囚徒困境中的更有效选择。《冲突解决杂志》24(3)，379–403 (1980)
3. Axelrod, R.：《合作的演化》。Basic Books，纽约 (1984)
4. Axelrod, R.：重复囚徒困境中策略的演化。载于：L.D. Davis（编），《遗传算法与模拟退火》第三章，第32–41页。Morgan Kaufmann，纽约 (1987)
5. Axelrod, R., Hamilton, W.D.：合作的演化。《科学》211，1390–1396 (1981)
6. Batali, J., Kitcher, P.：可选和强制性博弈中利他主义的演化。《理论生物学杂志》175，161–171 (1995)
7. Chellapilla, K., Fogel, D.B.：演化、神经网络、博弈与智能。《IEEE学报》87(9)，1471–1496 (1999)
8. Chong, S.Y., Ku, D.C., Lim, H.S., Tan, M.K., White, J.D.：演化神经网络学习黑白棋策略。载于：2003年演化计算大会（CEC’03）论文集，第2222–2229页。IEEE出版社，新泽西州皮斯卡特维 (2003)
9. Chong, S.Y., Tan, M.K., White, J.D.：观察神经网络学习黑白棋游戏时的演化过程。《IEEE演化计算汇刊》9(3)，240–251 (2005)
10. Chong, S.Y., Tiˇno, P., Ku, D.C., Yao, X.：提升协同演化学习中的泛化性能。《IEEE演化计算汇刊》16(1)，70–85 (2012)
11. Chong, S.Y., Tiˇno, P., Yao, X.：测量协同演化学习中的泛化性能。《IEEE演化计算汇刊》12(4)，479–505 (2008)
12. Chong, S.Y., Yao, X.：重复囚徒困境中的行为多样性、选择和噪声。《IEEE演化计算汇刊》9(6)，540–551 (2005)
13. Chong, S.Y., Yao, X.：多智能体交互中的多重选择与声誉机制。《IEEE演化计算汇刊》11(6)，689–711 (2007)
14. Darwen, P., Yao, X.：关于为重复囚徒困境演化鲁棒策略。载于：《演化计算进展》。人工智能讲义丛书，卷956，第276–292页 (1995)
15. Darwen, P., Yao, X.：增加遗传多样性是否能保持协同进化军备竞赛中局势升级。《基于知识的智能工程系统国际杂志》4(3)，191–200 (2000)
16. Darwen, P., Yao, X.：为何更多选择导致重复囚徒困境中合作减少。载于：2001年演化计算大会（CEC’01）论文集，第987–994页。IEEE出版社，新泽西州皮斯卡特维 (2001)
17. Darwen, P., Yao, X.：在具有中等合作水平的重复囚徒困境中协同演化：在导弹防御中的应用。《计算智能应用国际杂志》2(1)，83–107 (2002)
18. Fogel, D.B.：游戏中智能决策的演化。《控制论与系统：国际杂志》22，223–236 (1991)
19. Fogel, D.B.：在重复囚徒困境中演化行为。《演化计算》1(1)，77–97 (1993)
20. Fogel, D.B.：遭遇持续时间与重复囚徒困境中合作演化的关系。《演化计算》3(3)，349–363 (1996)
21. Fogel, D.B.：《Blondie24：AI边缘的博弈》。Morgan Kaufmann，加州旧金山 (2002)
22. Fogel, D.B., Hays, T.J., Hahn, S.L., Quon, J.：一个自学习的演化国际象棋程序。《IEEE学报》92(12)，1947–1954 (2004)
23. Frean, M.R., Abraham, E.R.：空间囚徒困境的投票者模型。《IEEE演化计算汇刊》5(2)，117–121 (2001)
24. Harrald, P.G., Fogel, D.B.：在重复囚徒困境中演化连续行为。《生物系统：囚徒困境专题》37，135–145 (1996)
25. Julstrom, B.A.：竞赛长度和噪声对重复囚徒困境中的互惠利他主义、合作及收益的影响。载于：第七届遗传算法国际会议（ICGA’97）论文集，第386–392页。Morgan Kauffman，加州旧金山 (1997)
26. Leimar, O., Hammerstein, P.：通过间接互惠实现合作的演化。《伦敦皇家学会学报B》268，745–753 (2001)
27. Lindgren, K.：简单动力学中的演化现象。载于：C.G. Langton、C. Taylor、J.D. Farmer、S. Rasmussen（编），《人工生命II》，第295–312页。Addison-Wesley，纽约 (1991)
28. Nisan, N., Roughgarden, T., Tardos, E., Vazirani, V.V.（编）：《算法博弈论》。剑桥大学出版社，纽约 (2007)
29. Nowak, M.A., Sigmund, K.：间接互惠的动力学。《理论生物学杂志》194，561–574 (1998)
30. Nowak, M.A., Sigmund, K.：通过形象评分实现间接互惠的演化。《自然》393，573–577 (1998)
31. Roberts, G., Sherratt, T.N.：通过增加投入发展合作关系。《自然》394，175–179 (1998)
32. Runarsson, T.P., Lucas, S.M.：协同进化与自我博弈时序差分学习在小棋盘围棋中的位置评估获取。《IEEE演化计算汇刊》9(6)，540–551 (2005)
33. Wahl, L.M., Nowak, M.A.：连续囚徒困境：I.线性反应策略。《理论生物学杂志》200(3)，307–321 (1999)
34. Yao, X.：演化人工神经网络。《IEEE学报》87(9)，1423–1447 (1999)
35. Yao, X., Darwen, P.：在多智能体环境中声誉有多重要。载于：1999年系统、人与控制会议（SMC’99）论文集，第575–580页。IEEE出版社，新泽西州皮斯卡特维 (1999)
36. Yao, X., Liu, Y., Lin, G.：更快的演化规划方法。《IEEE演化计算汇刊》3(2)，82–102 (1999)
