第12章 协同进化学习中的泛化能力

12.1 竞争性协同进化中性能分析简介

竞争性协同进化已被成功应用于游戏问题的求解[$2, 10, 11, 46, 55, 57$]，以及协同优化[$39$]和分类[$41, 60$]等相关问题求解模型下的学习过程。然而，竞争性协同进化学习已知存在一些病态问题，会对协同进化学习系统的搜索性能产生负面影响[$9, 24, 26, 28, 30, 49, 54, 59$]。我们在第5章中已经介绍并讨论了方案之间的成对关系中，由非传递性引发的特定循环结构如何导致这些病态问题。现在，通过对协同进化问题结构的形式化研究[$6, 7, 12$]，我们对这些协同进化病态现象有了更深入的理解。由于解与解之间的成对比较构成了定义、计算并在选择过程中使用相对适应度值的基本原语，因此这些现象对竞争性协同进化的实际操作具有根本性的意义。协同进化问题中的循环结构与协同进化过程中动力学的循环结构之间存在更深层次的联系。目前的理论研究[$29, 58$]仅仅开始揭示这些复杂的动力学，描述了当协同进化过程进行时，即便在诸如双人博弈的两纯策略等简单交互形式下，也会出现的种群配置或状态的丰富轨迹。我们对协同进化搜索的理解以及能够提供一定解释的场景，主要来自对协同进化系统测量结果的定量分析。在竞争性协同进化的相关文献中，循环动力学可能表现为种群在不同态之间的循环，这些态为对应某一特定解的过度专化种群成员状态，切换至对应其他特定解的其他过度专化种群状态，而在这些个体解之间则存在非传递性关系（例如“石头-剪刀-布”动力学）[$9, 24, 26, 28, 59$]。
在设计此类竞争性协同进化学习系统时，其影响远不仅仅在于缓解病态问题，还涉及确保搜索性能的有效性，且可从更广泛的角度实现协同进化搜索的性能提升。尤其是，协同进化搜索性能还依赖于协同进化算法（Coevolutionary Algorithms, CEAs）的一般框架中的其他组成部分。例如，解的表示方式 [16] 以及变异算子 [16, 24, 25] 会影响协同进化搜索的有效性，因为它们会改变搜索空间的本质。事实上，即便已获得一定理论见解，系统性且可控的计算研究仍被广泛应用于竞争性协同进化的研究中 [4, 12]。这就需要进一步的研究，专注于能够衡量并监控协同进化搜索进展的方法，以便在对问题结构及搜索过程尚不明确的情况下（如军备竞赛动态），能够监测更具创新性与复杂性的解的协同进化过程 [3, 18, 31, 56]。另一种方法则是从全局的角度分析协同进化学习性能的提升。因为那些被设计用于监控军备竞赛动态进展的工具并不适用于这种全局视角的分析，它们仅提供了解之间跨代的相对性能信息。

在本章中，我们介绍了一系列相关研究 [13–15]，这些研究在竞争性协同进化环境中采用了全局视角分析。我们的方法基于机器学习领域的泛化（generalization）概念，即关注学习系统性能的全局视角。泛化指的是学习系统在输入-输出映射形式上，能够对训练过程中未见的新输入做出最佳预测所需输出的能力。因此，能够输出良好泛化性能解的学习系统，是那些能够从少量训练数据样本中发现问题潜在特性的系统 [5]。和其他学习系统一样，竞争性协同进化在进化（训练）过程中同样利用少量测试案例样本，来引导解的搜索。然而，协同进化学习与一般学习系统、尤其是有监督设置下的学习系统在一个关键点上不同：在协同进化中，训练样本并非固定不变，而是会随着进化出的解的变化而动态调整。事实上，解与测试案例之间的特定相互作用及其各自的共适应构成了协同进化中竞争性学习的核心特征 [53, 54]。尽管如此，泛化框架仍可应用于竞争性协同进化学习系统，为性能分析提供全局视角。虽然泛化的概念在机器学习领域已被充分理解，但目前还缺乏理论框架能够明确指定协同进化学习系统针对所求解问题的泛化性能究竟为何。

以往的研究 [20, 64] 通常采用大量随机获得的测试案例样本来估算竞争性协同进化的泛化性能。然而，鉴于尚未精确定义“真实泛化性能”的含义，基于随机测试样本估算得到的泛化性能的准确性也无明确界定。为解决这一问题，我们首先提出了关于竞争性协同进化环境下泛化性能的理论框架——其中解与测试案例之间的相互作用被建模为竞争性博弈 [14]。为便于表达，我们聚焦于解与测试案例之间的对称两人博弈。特别说明，在本章余下部分，我们将采用简化术语，直接称实现纯粹博弈策略的智能体为（测试）策略。我们定义由竞争性协同进化产生的某策略的泛化性能为：该策略针对从策略空间抽取的测试策略时的期望性能或平均性能。由此可知，能很好泛化于某博弈的策略意味着它可以击败博弈中大量存在的对手测试策略。但需要注意的是，该理论框架是通用的，适用于许多需要协同优化的环境，其中解的性能比较需要基于测试型评估而非基于已明确定义的目标函数 [48, 50]。
本章其余部分分为两个主要的技术部分。第12.2节正式介绍了竞争性协同进化的一般化性能框架。我们提出了一种估算程序，该程序通过对一小部分随机测试策略计算平均性能，并推导出与某些无关分布的精度水平相对应的置信界。我们通过受控的计算研究，展示了如何构建与协同进化系统分析相关的多种一般化性能度量方法。在第12.3节中，我们将展示如何利用平均对局结果近似高斯分布的特性，对原本无关分布的一般化性能公式进行改进，并基于参数检验给出更紧致的界。正式分析需要利用关于生成随机测试策略的底层分布的对局结果的高阶（第二阶和第三阶）矩。这些高阶矩用于估计平均对局结果分布收敛到标准高斯分布的速度（即随样本量的增加）。本章最后将简要讨论这些定量分析工具如何用于辅助研究竞争性协同进化学习中一般化与多样性之间关系的确定。

12.2 协同进化学习中的一般化性能

在本节中，我们介绍我们在文献[14]中的研究工作，其中我们发展了竞争性协同进化中的一般化性能理论框架，展示了如何构建一般化性能度量的统计估算方法，并随后应用这些估算方法对协同进化学习进行严格的定量分析。第12.2.1节将正式定义竞争性协同进化中的一般化，如何利用一组随机测试策略样本进行统计估计，以及如何构建无关分布的置信界，从而使得对一般化性能估计的准确性可以做出统计推断。接下来，我们将介绍各种有用的性能度量方法，比如针对高性能测试策略的有偏样本下的一般化，以及策略集成（ensemble）的一般化性能度量，这将在第12.2.2节阐述。随后，第12.2.3节将介绍各种计算实验，用于验证所提框架，并展示这些估算方法在为竞争性博弈的协同进化学习提供严格定量分析时的实际应用价值。
12.2.1 泛化性能的理论框架

在竞争性共进化学习中，解的质量是相对于其在测试用例中的表现来确定的。解与测试用例之间的交互被表述为一个双人博弈。假设这种交互（博弈过程）是对称的，因此策略$i$和测试（对手）策略$j$都取自有限的不同策略集合$\mathcal{S}$。设$i$对$j$的博弈结果为$G_i(j)$（反之，也可以定义$j$对$i$的博弈结果为$G_j(i)$）。策略$i$的绝对质量通过其在解决由对手策略$j$提供的测试中的期望表现来衡量。设从$\mathcal{S}$中选取单个测试策略的过程用随机变量$J$表示，其取值$j \in \mathcal{S}$的概率为$\mathbb{P}_{\mathcal{S}}(j)$。有些策略比其他策略更受青睐，或者先验地认为所有策略被选中的概率相等。我们将策略$i$的真实泛化性能$G_i$定义为随机变量$G_i(J)$的均值，即
$$
G_i = \mathbb{E}_{\mathbb{P}_{\mathcal{S}}}[G_i(J)],
$$
其计算方式为[14]
$$
G_i = \sum_{j=1}^{M} \mathbb{P}_{\mathcal{S}}(j)\ G_i(j).
$$
例如，当所有策略被选择的概率相等时，有
$$
G_i = \frac{1}{M} \sum_{j=1}^{M} G_i(j).
$$

在实际应用中，把等式(12.1)直接作为泛化性能度量会遇到三个相关的困难。首先，即使对于简单的博弈，$G_i$的闭式表达式也很难获得，因为$G_i(j)$的博弈结果事先未知。其次，$\mathbb{P}_{\mathcal{S}}$在$\mathcal{S}$上的概率分布通常是未知的，实际中只能从$\mathcal{S}$中采样。采样依赖于所采用的具体策略表示方式，因此某些策略$j$虽存在，但由于$\mathbb{P}_{\mathcal{S}}(j) = 0$，不会被纳入$G_i$的计算中。第三，即使对于简单博弈，直接通过穷举计算$G_i$也是不可行的。例如，考虑$n$-选项的迭代囚徒困境（IPD）博弈，采用确定性和反应式记忆一阶策略。所有唯一策略的总数为$n^{n^2 + 1}$ [16]，其增长速度快于指数型。例如，$2^{2^2 + 1} = 2^5 = 0.32 \times 10^2$，$3^{3^2 + 1} = 3^{10} \approx 0.59 \times 10^4$，$4^{4^2 + 1} = 4^{17} \approx 0.17 \times 10^{10}$，以此类推。直接计算$G_i$很快就会变得在计算上不可行。
相反地，我们利用 $G_i = {\mathbb{E}}_{{\mathbb{P}}_{\mathcal{S}}}[G_i(J)]$ 的公式，并采用一种程序，使用从 $\mathcal{S}$ 中独立同分布（iid）采样得到的 $N$ 个测试策略组成的随机样本 $S_N$，对其真实值进行估计。策略 $i$ 相对于样本 $S_N$ 的泛化性能估计值 $\hat{G}_i(S_N)$ 计算如下 [14]：

\[
\hat{G}_i(S_N) = \frac{1}{N} \sum_{j \in S_N} G_i(j)
\]

在本章的其余部分，我们采用简化记号 $\hat{G}_i$ 代表 $\hat{G}_i(S_N)$。我们也将 $\hat{G}_i(S_N)$ 称为策略 $i$ 泛化性能的统计估计量。泛化性能估计的准确性问题在于确定 $\hat{G}_i$ 与 $G_i$ 之间的接近程度。对于给定的精度水平 $\epsilon > 0$，这意味着需要判断估计值与真实值之差 $|\hat{G}_i - G_i|$ 是否小于 $\epsilon$。如果 $|\hat{G}_i - G_i| < \epsilon$ 成立，则可以认为在指定的精度水平下，$\hat{G}_i \approx G_i$。然而，$|\hat{G}_i - G_i|$ 是未知的，因为 $G_i$ 本身未知。因此，替代方法是判断 $\hat{G}_i$ 与 $G_i$ 的接近概率。要做出统计上的判断，需要确定对于一个利用随机样本 $S_N$ 获得的 $\hat{G}_i$，有多大的概率 $|\hat{G}_i - G_i| \geq \epsilon$。这个问题可以通过切比雪夫定理（Chebyshev’s Theorem）[36] 来解答。需要注意，差值 $D_N = \hat{G}_i - G_i$ 是一个随机变量，因为 $\hat{G}_i$ 是随机变量。根据 $D_N$ 的定义，可以得到如下结果 [14]：

\[
{\mathbb{P}}_N(|\hat{G}_i - G_i| \geq \epsilon) \leq \frac{\sigma^2_i}{N \cdot {\epsilon}^2}
\]

对于策略 $i$，在本章其余部分我们用简化记号 $\sigma^2$ 代替 $\sigma^2_i$。此外，

\[
\sigma^2 = {\mathbb{E}}_{P_{\mathcal{S}}}\Big[ \big( G_i(J) - G_i \big)^2 \Big] 
= {\mathbb{E}}_{{\mathbb{P}}_{\mathcal{S}}}\Big[ \big( G_i(J) - {\mathbb{E}}_{{\mathbb{P}}_{\mathcal{S}}}[G_i(J)] \big)^2 \Big] 
= \mathrm{var}_{{\mathbb{P}}_{\mathcal{S}}}[G_i(J)].
\]

尽管二阶矩 $\sigma^2$ 也是未知的，我们利用其可以被上界所限制的事实。一般而言，对于在区间 $[G_{\mathrm{min}}, G_{\mathrm{max}}]$ 上分布的随机变量 $G_i(J)$，其中 $G_{\mathrm{max}}$ 和 $G_{\mathrm{min}}$ 分别是 $G_i(J)$ 的最大值和最小值，当一半的概率质量分布在 $G_{\mathrm{min}}$，另一半分布在 $G_{\mathrm{max}}$ 时，方差 $\sigma^2_{\mathrm{max}}$ 取得最大值。这时，均值为 $(G_{\mathrm{min}} + G_{\mathrm{max}})/2$，标准差为 $\sigma_{\mathrm{max}} = (G_{\mathrm{max}} + G_{\mathrm{min}})/2$ [40]。记 $R = G_{\mathrm{max}} - G_{\mathrm{min}}$ 为随机变量 $G_i(J)$ 的变化范围。则有

\[
\mathrm{var}_{{\mathbb{P}}_{\mathcal{S}}}[G_i(J)]
\]

的上界为

\[
\sigma^2_{\mathrm{max}} = R^2 / 4
\]

这一点可用于证明如下理论结果（引理 12.1）。
引理12.1（[14]） 对于某个策略$i$，设$\hat{G}_i$为对$N$个随机测试策略的泛化性能估计，$G_i$为其真实泛化性能。考虑$|\hat{G}_i - G_i|$这一绝对误差，它是一个在紧致区间$[G_{\mathrm{min}}, G_{\mathrm{max}}]$上的随机变量，区间长度为$R = G_{\mathrm{max}} - G_{\mathrm{min}}$。那么，对于任意正数$\epsilon > 0$，有
$$
{\mathbb{P}}_N(|\hat{G}_i - G_i| \geq \epsilon) \leq \frac{R^2}{4N{\epsilon}^2}.
$$
（12.4）需要注意的是，由式（12.4）给出的切比雪夫界是分布无关的。特别地，在推导切比雪夫界时，并没有对$G_i(J)$的分布做任何假设。对于用$N$个测试策略的随机样本计算得到的估计$\hat{G}_i(S_N)$，可以就其精度作出统计性判断。对式（12.4）取补集，可以宣称以概率至少为$p = 1 - R^2/(4N{\epsilon}^2)$，或置信度为$100p\%$，有$|\hat{G}_i - G_i| < \epsilon$，其中精度水平$\epsilon > 0$（见[14]）。我们可以如下说明切比雪夫界。令$|D_N|' = |D_N| / R = |\hat{G}_i - G_i| / R$，$\epsilon' = \epsilon / R$。这样的归一化可以将式（12.4）表达为更简洁的形式：
$$
{\mathbb{P}}_N(|D_N|' \geq \epsilon') \leq \frac{1}{4N{\epsilon'}^2},
$$
即
$$
{\mathbb{P}}_N(|D_N|' \geq \epsilon') \leq \frac{1}{4N{\epsilon'}^2}, \qquad (12.5)
$$
随后可以将切比雪夫界绘制为${\mathbb{P}}_N(\epsilon') = 1/(4N{\epsilon'}^2)$在${\mathbb{P}}-\epsilon'$空间中的点。图12.1展示了在$\epsilon' \in [0.01, 0.1]$区间，针对不同$N$的切比雪夫上界${\mathbb{P}}_N$的曲线。每一条曲线${\mathbb{P}}_N$给出了${\mathbb{P}}_N(|D_N|' \geq \epsilon')$的上界。需要注意的是，${\mathbb{P}}_N(\epsilon')$与$N$和$\epsilon'$成反比。对于任意$\epsilon'$，获得更低的切比雪夫上界需要$N$越来越大（参见图12.1）。实际应用中，通常会尽可能大地指定$N$以获得$\hat{G}_i$，然后在置信度和精度水平之间进行权衡。更重要的是，我们通过如下三点强调该框架的通用性（见[14]）：

1. 该理论框架不依赖于博弈自身的复杂性，因为它与策略空间的大小无关，也与策略空间中策略的分布无关。
2. $G_{\mathrm{max}}$和$G_{\mathrm{min}}$通常是先验已知的，因为它们由博弈本身规定，这意味着我们总可以对式（12.4）获得一个上界。
0.8 0.6 0.4 0.2 0.1 0.05 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  
概率，$p$  
精度值，$e'$  
$pN=500$  
$pN=1000$  
$pN=5000$  
$pN=10000$  

图12.1 展示了不同样本量$N$下Chebyshev界${\mathbb{P}}_N$在精度值$e'$属于区间$[0.01, 0.1]$时的多组曲线[14]。

3. 该框架与学习算法无关，因为该界对策略空间内的任何策略均成立。

12.2.2 泛化性能度量的构建

在本节中，我们展示了不同泛化性能度量的构建方法，这些度量可应用于竞争性协同进化学习系统中，提供不同方面的定量分析。我们关注这些性能度量的三个不同方面。首先，在竞争性协同进化环境中，性能的概念依赖于解与测试用例之间交互结果$G_i(j)$的具体定义。在双人对抗博弈中，可以将性能定义为胜负博弈结果的上下文。需要注意的是，经典博弈论[45]将零和博弈定义为：由两名对抗玩家分别采用任意一对策略所得到的结果之和总为零。在这种情况下，$G_i(j)$的胜利必然导致$G_j(i)$的失败，平局则双方得分均为零。对于非零和博弈，胜利博弈结果的一种具体含义是：主导方$i$所获得的博弈收益高于对手$j$所获得的收益。
我们的理论框架（引理12.1）可以结合多种博弈结果的定义，只要这些结果被限定在一个取值区间 $[G_{\mathrm{min}}, G_{\mathrm{max}}]$。为了说明我们的框架，我们基于任意两玩家博弈，构建了两种不同泛化性能测度的统计量估计方法。设 $g(i, j)$ 表示策略 $i$ 在与策略 $j$ 的对弈中所获得的累计收益（或平均收益），策略 $j$ 同时获得 $g(j, i)$。我们将胜负博弈的结果定义为：

$$
G_{\mathrm{W}}(i, j) = 
\begin{cases}
C_{\mathrm{win}}, & \text{如果}\ g(i, j) > g(j, i), \\
C_{\mathrm{lose}}, & \text{否则},
\end{cases}
$$

其中，$C_{\mathrm{win}}$ 与 $C_{\mathrm{lose}}$ 是任意常数，满足 $C_{\mathrm{win}} > C_{\mathrm{lose}}$。在随后的实证研究中，我们令 $C_{\mathrm{win}} = 100$，$C_{\mathrm{lose}} = 0$。基于这种胜负博弈结果的泛化性能估计为 [14]：

$$
\hat{G}_{\mathrm{W}}(i) = \frac{1}{N} \sum_{j \in S_N} G_{\mathrm{W}}(i,j)。
$$

需要注意的是，$G_{\mathrm{W}}(i)$ 表示真实的泛化性能。然而，也可以用博弈收益来定义泛化性能。假定对于任意策略对 $i, j \in \mathcal{S}$，博弈收益非负，即 $g(i, j) \geq 0$。则基于博弈收益聚合的泛化性能估计为 [14]：

$$
\hat{G}_{\mathrm{A}}(i) = \frac{1}{N} \sum_{j \in S_N} G_{\mathrm{A}}(i, j)。
$$

同理，$G_{\mathrm{A}}(i)$ 也表示对应的真实泛化性能。举例说明，对于囚徒困境（IPD）博弈，博弈结果是每一步的平均收益，此时 $G_{\mathrm{A}}(i, j) = g(i, j) \in [G_{\mathrm{min}}, G_{\mathrm{max}}]$，其中 $G_{\mathrm{max}} = T$ 表示因背叛诱惑获得的收益，$G_{\mathrm{min}} = S$ 表示“傻瓜”选择所获收益或被利用时的收益。

泛化性能测度的第二个方面与以带偏样本测试策略集进行性能评估问题相关。这涉及策略空间 $\mathcal{S}$ 上的概率分布 $\mathbb{P}_\mathcal{S}$，而该分布通常是未知的，并可能随着所考虑的竞赛设置发生变化。以往的研究采用了各种启发式方法来获得这样的带偏样本，包括利用人类专家知识 [2, 35]，这种知识通常难以获得。此外，这些人类专家带偏样本通常样本量很小，因此在用于估计泛化性能时，可能会引入可扩展性问题，并由于测试策略与通过学习系统产生的策略在能力上的巨大差异，使得这类测度过于粗糙。
另一种方法是从策略空间中采样，以获得这些有偏测试样本。然而，在实际操作中这非常困难，因为在策略空间中缺乏“优秀度度量”（goodness metric）的概念，因此除非对于那些特意构建了此类度量空间的抽象游戏，大多数游戏都无法定位并以更高概率采样高表现力的测试策略。我们提出了一种基于简单启发式的流程来获得有偏的测试样本。多次部分枚举搜索扩展了我们在[19]中的早期方法。该流程如算法12.1所示。

算法12.1 多次部分枚举搜索 [14]  
输入：$Y$ 候选测试策略种群，$C$ 有偏测试策略样本数量，$N$ 单次部分枚举搜索的种群规模  
输出：$S$ 有偏测试策略样本集

1: procedure MULPARSEARCH($Y$, $C$, $N$)  
2: $r := 1$  初始化运行计数  
3: $S := \mathrm{initialize}(Y)$  初始化$S$  
4: while $r \leq C$ do  
5: $S_r := \mathrm{PARSEARCH}(N)$ 单次部分枚举搜索  
6: $r := r + 1$  
7: end while  
8: return $S$  
9: end procedure  

10: procedure PARSEARCH($N$)  
11: $Q := \{(q, f)_i, i = 1, \ldots, N \}$  随机采样$q \in S$，初始化得分$f$为零  
12: $\mathrm{roundrobinTour}(Q)$  在$Q$上进行循环赛以获得累计得分$f$  
13: $\mathrm{ascendingSort}((Q, f))$  按得分$f$从小到大排序$(q, f)_i$  
14: return $Q[N].q$  
15: end procedure  

我们要求每次部分枚举搜索中的种群规模$N$要大于协同进化运行中能够获得的不重复策略的最大数量，即 $N > (\mathrm{generation} \times \text{population \ size \ of \ coevolution})$。其启发式理由在于，相较于直接采样，学习系统在策略空间中搜索高表现力策略的能力更强。多次独立重复部分枚举搜索$C$次，旨在获得比[19]方法——即从一次运行中获得$C$个测试策略样本——更加多样化的高性能测试策略样本。重复部分枚举搜索可能有助于克服由于部分枚举搜索某种种群采样不佳而导致获得低表现测试策略的问题。

我们引入和讨论的泛化性能测量的第三个也是最后一个方面，涉及个体与种群层面的测度。在大多数情况下，分析通常聚焦于协同进化种群中表现最好的策略。然而，并没有必要将种群的最优协同进化策略作为唯一代表，特别是在博弈类应用中，即便是在最优策略之间，也可能存在不传递性（即，它们形成强连通分量）。我们可以选择种群中表现最优的$U$个协同进化策略，记为$\mathrm{SPOP}_{U} = \{\mathrm{spop}_1, \ldots, \mathrm{spop}_U \}$（例如，按其相对适应度排序）。在此，我们为协同进化学习系统[14]引入了三种不同的泛化性能度量方法：
(i) 最优值 $\mathrm{Best}(G_{\mathrm{SPOP}_{U}}) = \hat{G}_{\mathrm{spop}_1}$。

(ii) 平均值 $\mathrm{Avg}(G_{\mathrm{SPOP}_{U}}) = \frac{1}{U}\left(\sum_{l}^{U} \hat{G}_{\mathrm{spop}_l}\right)$。

(iii) 集成评估 $\mathrm{Ens}(G_{\mathrm{SPOP}_{U}}) = \frac{1}{N} \left( \sum_{j \in S_N} \mathrm{min} \left( \sum_{l}^{U} G_{\mathrm{spop}_l}(j), G_{\mathrm{max}} \right) \right)$。

前两种度量方式较为直接。$\mathrm{Best}(G_{\mathrm{SPOP}_{U}})$ 表示相对于子种群 $\mathrm{SPOP}_{U}$ 的最优策略的泛化性能。$\mathrm{Avg}(G_{\mathrm{SPOP}_{U}})$ 指的是 $\{\mathrm{spop}_1, \ldots, \mathrm{spop}_U \}$ 各个个体策略泛化性能的平均值。

然而，集成度量 $\mathrm{Ens}(G_{\mathrm{SPOP}_{U}})$ 假设 $\mathrm{SPOP}_{U}$ 中协同进化得到的策略集合配备了一个完美的门控机制 [22]。该度量方式意味着集成体总能针对每个测试策略选择 $\mathrm{SPOP}_{U}$ 中表现最佳的那一个协同进化策略。例如，考虑包含五个测试策略的情形。如果 $\mathrm{spop}_1$ 在前两个测试策略上表现最佳，而 $\mathrm{spop}_2$ 在后三个测试策略上表现最佳，则这两个演化策略的集成体会在全部测试策略上取得最佳表现，即对前两个测试策略选择 $\mathrm{spop}_1$，对其他测试策略选择 $\mathrm{spop}_2$。需要注意的是，该度量方式仅适用于 $G_{\mathrm{W}}(i)$。该度量方式用于判断整个种群的泛化能力是否优于单个个体策略。

12.2.3 Chebyshev界在竞争性协同进化中的计算研究
在本小节中，我们展示了在文献 [14] 中所进行的多项计算实验，以展示切比雪夫界（Chebyshev’s bound）在竞争共进化学习系统中的应用。首先，我们介绍一项旨在验证切比雪夫界并阐释所提出的基于随机测试样本 $S_N$ 的泛化性能统计估计方法机制的计算实验。我们以囚徒困境（IPD）博弈为例，具体地，采用具有确定性和反应性的记忆一阶三选策略（three-choice IPD with deterministic and reactive memory-one strategies）。已有多种策略表示方法被应用，例如基于比特串编码的查找表表示 [2]、有限状态机（Finite State Machines）[32–34] 和人工神经网络（Artificial Neural Networks, ANNs）[16, 23, 35, 38]。在本研究中，我们采用了在文献 [16] 中提出的直接查找表策略表示方法。图 12.2 展示了直接查找表表示方法，该方法实现了三选 IPD 策略，考虑了个体与对手上一轮的联合动作，其中 $m_{ij},\ i,j = 1,2,3$ 表示在索引 $i$ 和 $j$ 分别对应代理或玩家自身和对手上一回合选择的情况下应做出的选择。首次动作 $m_{\mathrm{fm}}$ 是独立规定的。对于三选情形，表中每个元素的取值范围为 $\{-1,0,+1\}$。采用直接查找表表示法的主要优势在于其诱导了有限、离散的策略空间 $\mathcal{S}$，从而简化并便于对共进化学习泛化性能的直接研究。我们假定在策略空间 $\mathcal{S}$ 上服从均匀概率分布 ${\mathbb{P}}_{\mathcal{S}}$。由于策略空间包含 $M = 59049$ 个唯一策略，因此可以精确计算真实的泛化性能 $G_i$。而对于四选 IPD 博弈则不可行，对于二选 IPD 博弈则是平凡的情况。本研究采用如下参数：$T = 5$、$R = 4$、$P = 1$、$S = 0$，并采用如下线性插值公式生成 $n$ 选 IPD 博弈的收益矩阵：
$$
p_{\mathrm{A}} = 2.5 - 0.5c_{\mathrm{A}} + 2c_{\mathrm{B}}, \quad -1 \leq c_{\mathrm{A}}, c_{\mathrm{B}} \leq 1, \tag{12.12}
$$
其中 $p_{\mathrm{A}}$ 表示在玩家 A 选择联合动作的合作水平分别为 $c_{\mathrm{A}}$（A 方）和 $c_{\mathrm{B}}$（B 方）时，A 所获得的收益。该离散合作水平对应 $n$ 种选择，其中完全合作为 $+1$，完全叛变为 $-1$ [16, 23–25, 63]。例如，三选与四选 IPD 博弈的收益矩阵分别如图 12.3 和图 12.4 所示 [16]。所有的 IPD 博弈均设定为固定对局长度，即 150 次迭代。

图 12.2 展示了考虑三选情况下的确定性及反应性记忆一阶 IPD 策略的直接查找表表示方法。反应部分由所示的 $3 \times 3$ 矩阵表示，首次动作 $m_{\mathrm{fm}}$ 未在图中呈现。
在以下实验中，计算了从$\mathcal{S}$中随机抽取的4000个策略的泛化性能的估计值$\hat{G}_i(S_N)$和真实值$G_i$。统计了这4000个策略中满足$|D_N|' > \epsilon'$的比例。该比例对应于实际概率$\mathbb{P}_N(|D_N|' \geq \epsilon')$。对于不同的$N$值（大致为$M$的$0.02\%$、$0.1\%$、$0.2\%$、$1\%$、$2\%$、$10\%$和$20\%$，分别对应$N=10, 50, 100, 500, 1000, 5000, 10000$），每个$N$都独立随机抽取50个测试集$S_N$进行实验。注意，对于每个样本$S_N$，不会有策略被重复选取。图12.5展示了以$G_W(i)$定义的泛化性能的经验结果，并用$\hat{G}_W(i)$进行估算。可以观察到比例的变化。

图12.3  两人三选IPD博弈的回报矩阵。矩阵中的每个元素表示行方（玩家A）的得分。  
图12.4  两人四选IPD博弈的回报矩阵。矩阵中的每个元素表示行方（玩家A）的得分。  
1 0.8 0.6 0.4 0.2 0.05 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 概率$p$ 精度值$\epsilon'$

$N=10 \quad N=50 \quad N=100 \quad N=500$  
(a) $N = \{10, 50, 100, 500\}$  
1 0.8 0.6 0.4 0.2 0.05 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 概率$p$ 精度值$\epsilon'$  

$pN=500 \quad pN=1000 \quad N=500 \quad N=1000 \quad N=5000 \quad N=10000$  
(b) $N = \{500, 1000, 5000, 10000\}$  

图12.5  展示了在采用大小为$N$的随机测试策略样本$S_N$计算$\hat{G}_W(i)$时，对于$\epsilon' \in [0.01, 0.1]$，有$|D_N|' > \epsilon'$的概率。每个折线图通过对50个独立样本$S_N$的实验结果取平均获得，误差棒表示95%的置信区间。$P_N$给出了对应不同$N$的切比雪夫界限曲线[14]。可见，对于$e'\in[0.01, 0.1]$，随$N$从10增至10000，$\{|D_N|' > \epsilon'\}_i, i = 1, \ldots, 4000$的比例逐渐减小。事实上，实测概率曲线通常低于由切比雪夫界限$P_N(|D_N|' \geq \epsilon')$给出的值。

接下来，我们分析针对单个策略的$G_i(j) - G(i)$的实证分布（直方图），其中这些策略及测试样本均为随机选取。进一步考察发现，对于四种不同的策略，这些实证分布始终呈双峰分布（即具有两个不同的峰），原因在于某个策略$i$对策略$j$要么胜、要么负。值得注意的是，随着样本量$N$的增加，这些分布曲线形态并无本质变化。

这些结果有两点重要意义：首先，采用较小$S_N$得到的估计和使用较大$S_N$的准确性相当；其次，从较小样本$S_N$出发，估算结果对于样本规模变化表现出高度稳定性。这一现象并不令人意外，因为切比雪夫界限$P_N(|D_N|' \geq \epsilon')$以$\sigma^2_i = R^2/4$为最坏情况进行估计[14]。

接下来，我们说明如何将泛化性能评估框架应用于竞争型协同进化学习系统的严格定量分析。尤其关注简单IPD博弈（如两选和三选）场景下，能够直接计算策略的真实泛化性能。此类计算研究表明，通过较小样本$S_N$即可对协同进化学习系统生成的策略的泛化性能进行良好估计。更为关键的是，可以针对协同进化种群开展多种泛化性能度量，这有助于对学习过程的深入分析。在后续实验中，我们采用基于经典进化规划（Evolutionary Programming, EP）和锦标赛选择的单种群竞争协同进化框架，全部种群大小为$2\lambda$（参见1.2.4节）。算法12.2描述了这一经典协同进化学习方法（CCL）[16]。实验中采用简单变异算子，直接对父代产生子代，即随机将直接查找表$m_{\mathrm{fm}}$和$m_{ij}$中某一元素的原有选择替换为另外$n-1$个选择之一，概率为$1/(n-1)$。固定变异率$p_{\mathrm{m}}$。某策略的适应度$\boldsymbol{X}[i].f$取其在循环赛中对所有对手比赛所得的平均得分，进化过程持续足够长的固定代数（例如$b_{\mathrm{gen}} = 300$），以观察CCL的泛化性能变化。所有实验均重复30次独立运行。
可以根据文献 [14] 的结果，对CCL在$G_{\mathrm{W}}(i, j)$泛化性能方面（三种衡量方式：最佳、平均和集成）进行如下观察。需要注意的是，对于两种IPD博弈的CCL，集成测度始终高于最佳和平均测度。然而，这一差异并不显著，说明共同进化种群中的策略大多较为相似。CCL的泛化性能在二选择和三选择IPD博弈中有所不同。对于二选择IPD的CCL，$G_{\mathrm{W}}(i)$测度随着进化过程的进行而下降。而对于三选择IPD的CCL，$G_{\mathrm{W}}(i)$测度在整个进化过程中基本保持在相似水平。对于较为简单的二选择IPD博弈，CCL种群通常会过度专注于更具合作性的策略（例如全部合作，见算法12.2 经典共进化学习（CCL）[16]）。
输入：Y人口，包含$\lambda$个候选策略  
$b_{\text{gen}}$终止准则——固定的迭代代数  
输出：X人口，包含$\lambda$个共同进化后的策略

1: 过程 CCL($Y$, $b_{\text{gen}}$)  
2: $t := 0$ \quad 初始化时间步  
3: $X_p := \text{initialize}(Y)$ \quad 初始化父代种群$X_p$  
4: 当 $t < b_{\text{gen}}$ 时 do  
5: \hspace{0.5cm} $X_o := \text{variation}(X_p)$ \quad 生成子代种群
   \hspace{0.5cm} 对于 $i = 1, \ldots, \lambda$, $X_o[i].x := \text{mutation}(X_p[i].x)$  
6: \hspace{0.5cm} $X := \text{combine}(X_p, X_o)$ \quad $X = X_p \cup X_o$  
7: \hspace{0.5cm} $\text{roundrobinTour}(X)$ \quad 在$X$上进行循环赛，以获得累计博弈收益$X[i].f$  
8: \hspace{0.5cm} $X_p := \text{selectionTour}(X)$ \quad 竞赛选择：在$X$中选择表现最优的一半个体  
9: \hspace{0.5cm} $t := t + 1$  
10: 结束循环  
11: 返回 $X_p$  
12: 结束过程

采用$\text{GW}(i)$测度时，这种演化过程会产生泛化能力较差的策略。特别地，仅当$g(i, j) > g(j, i)$时，策略$i$才算赢得博弈结果。合作型策略往往会被背叛型策略利用，因此在IPD博弈中的收益低于其对手，即$g(i, j) < g(j, i)$，导致泛化性能$\text{GW}(i)$较差[14]。对于选择较少的IPD博弈，CCL方法更容易对天真的合作者过度专化，其原因在于：可选空间$\mathcal{S}$中，天真型合作者所占比例更高。

以两选项IPD中“全合作”策略为例，其首步动作$m_{\mathrm{fm}} = +1$，无论自身或对手上一轮采取的联合动作为何，都选择完全合作。其查找表配置如下：

$$
\left(
\begin{array}{cc}
+1 & +1 \\
* & *
\end{array}
\right),
$$

其中$*$可取$+1$或$-1$，对应两选项IPD。当$m_{\mathrm{fm}} = +1$且第一行均为$+1$时，该策略总是选择完全合作，因为只有第一行会被访问。这样的表共有$2^{2}$种组合，因此$\mathcal{S}$空间中“全体合作”策略的比例为$2^2 / 2^5$。同理，对于三选项IPD，“全合作”策略的$m_{\mathrm{fm}} = +1$，查找表为：

$$
\left(
\begin{array}{ccc}
+1 & +1 & +1 \\
* & * & * \\
* & * & *
\end{array}
\right),
$$
$\begin{pmatrix}
+1\quad +1\quad +1\quad *\ *\ *\ *\ *\ *
\end{pmatrix}$，其中 $*$ 可以取 $+1$、$0$ 或 $-1$，对应三选囚徒困境（three-choice IPD）中的三种选择。在策略空间 $\mathcal{S}$ 中，全体合作（all cooperate）策略所占比例为 $3^6/3^{10}=36/310$。对于$n$选囚徒困境（n-choice IPD），全体合作策略的比例由 $n^{(n^2 - n)} / n^{(n^2 + 1)} = 1/n^{(n + 1)}$ 给出。随着 $n$ 的增大，全体合作策略的比例以指数速度下降。对于两选囚徒困境，全体合作策略的比例为 $12.5\%$。对于三选囚徒困境，该比例大约为 $1\%$。

先前的多项研究表明，若将循环赛（round-robin tournament）累积或平均博弈收益作为适应度值进行协同进化，更容易产生由互相合作策略组成的种群。CCL机制[16]同样具有此特性。在互助合作体（mutual cooperators）组成的种群中，CCL无法区分不同的互助合作策略，因为这些策略的适应度值非常接近。当CCL无法区分这些互助合作策略时，种群更容易漂变至“天真合作”（naive cooperators）上。这也反映在我们的实验结果中——对于两选囚徒困境，平均而言约有$12\%$的种群为全体合作；而对于三选囚徒困境，这一比例约为$0.2\%$。

尽管如此，本计算研究表明，泛化性能度量能够提供定量分析，这与策略空间$\mathcal{S}$的深层结构分析、合作性对弈组合数学及既有关于协同进化趋向合作对弈的理解相呼应。

本小节最后，我们在三选囚徒困境的CCL情形下比较了测试策略偏向采样（biased sample）与无偏采样（unbiased sample）情况下的泛化性能。我们实现了算法12.1，并调用过程$\mathrm{MULPARSEARCH}(\boldsymbol{Y},C,N)$，其中$C = 20$，以获得包含20个测试策略的样本。我们考察了不同种群规模 $N \in \{1,4,10,50,100,10000\}$ 的情形。只有$N=10000$满足搜索策略数量超过CCL能达到的最好水平（即 $10000 > \mathrm{b}_{\mathrm{gen}} \times 2\lambda = 300 \times 20 = 6000$）。选择如此大的$N$，是为了研究其在多重部分枚举搜索过程中，生成有偏且多样化的高性能测试策略样本对协同进化策略带来的挑战性影响。我们用$\hat{G}^{\mathrm{B}}_i$表示基于有偏测试策略样本估计得到的泛化性能测度。实验结果总结于图12.6。总体来说，调用$\mathrm{MULPARSEARCH}(\boldsymbol{Y},C,N)$时增大$N$能够得到更难战胜的高性能测试策略样本。例如，相同协同进化策略下，$\hat{G}^{\mathrm{B}}_i$测度低于$G_i$测度。两种测度间的差异，尤其在使用更大$N$时、由$\mathrm{MULPARSEARCH}(\boldsymbol{Y},C,N)$产生的有偏样本计算$\hat{G}^{\mathrm{B}}_i$时尤为显著，如图12.6b所示[14]。
10 20 30 40 50 60 70  
0 50 100 150 200 250 300  
泛化性能 进化代数 $GW(i)$  
MS00001 MS00004 MS00010  
(a)  
0 10 20 30 40 50 60 70  
0 50 100 150 200 250 300  
泛化性能 进化代数 $GW(i)$  
MS00050 MS00100 MS10000  
(b)  
图12.6 在整个进化过程中，CCL中$GW(i)$与$\hat{G}_{\mathrm{W}}^{\mathrm{B}}(i)$的比较，取30次独立运行的平均值（仅“最佳”测量）。(a) 多个部分枚举搜索，种群规模为1、4和10。(b) 多个部分枚举搜索，种群规模为50、100和10000 [14]。

12.3 在协同进化学习中提升泛化性能估计

本节介绍我们在[13, 14]中为提升竞争性协同进化中泛化性能统计估计器所做的理论研究，特别关注其计算成本，因计算成本与用于计算这些估计的测试策略数量成正比。首先，在第12.3.1节中，我们将展示如何通过对方差$\sigma^2_i$进行第二次估计（该值可能远低于切比雪夫界中的最坏情况$R^2/4$）来改进统计估计器。关键地，在第12.3.2节中，我们将直接讨论切比雪夫界中的这一问题，并演示如何利用这些统计估计器的一个潜在性质（即平均博弈结果近似高斯分布），基于参数检验获得更为紧致的界。我们可以利用这一形式化理论对策略进行统计性比较检验，这将在第12.3.3节介绍。随后，第12.3.4节将展示多项计算实验，以验证这些理论研究，并说明这些改进的估计器如何用于分析更复杂竞争博弈中的协同进化学习。  

12.3.1 通过估计方差$\sigma^2$获得更紧的切比雪夫界

回顾策略$i \in \mathcal{S}$的泛化性能$G_i$定义为针对随机测试策略$J$取$j \in \mathcal{S}$时的期望平均性能（博弈结果）$\mathbb{E}_{\mathbb{P}_{\mathcal{S}}}[G_i(J)]$。即$G_i = \mathbb{E}_{\mathbb{P}_{\mathcal{S}}}[G_i(J)]$，且$G_i(J)$的取值落在有限区间内。在接下来的内容中，我们将演示如何通过对$G_i(J)$方差的再次估计（这一方法已在[14]中提出），改进切比雪夫界。该论证在技术上较为复杂，为简化表述，我们将$G_i(J)$视为随机变量$X$，其底层分布$\mathbb{P}_X$定义在实数的紧区间$X \in [a, b]$上。对于$N$个观测值$x_1, x_2, \ldots, x_N$，$X$的经验均值$\hat{\mu}_N$定义为
$\hat{\mu}_N = \frac{1}{N} \sum_{j = 1}^{N} x_j$

其中，真实均值$\mathbb{E}_{\mathbb{P}_X}[X]$和真实方差$\sigma^2$分别定义为
\[
\mathbb{E}_{\mathbb{P}_X}[X] = \mu, \\
\sigma^2 = \mathbb{E}_{\mathbb{P}_X}\left[\left(X - \mathbb{E}_{\mathbb{P}_X}[X]\right)^2\right].
\]
对于区间$[a, b]$上的随机变量$X$，其最大方差为$\sigma^2_{\mathrm{max}} = R^2_X / 4$，其中$R_X = b-a$。对于任意正的$\epsilon$，可以利用切比雪夫不等式得到：
\[
\mathbb{P}_N(|\hat{\mu}_N - \mu| \geq \epsilon) \leq \frac{R^2_X}{4N \cdot \epsilon^2}.
\]

我们希望获得比切比雪夫界限更加紧的上界。如果$\mu = \mathbb{E}_{\mathbb{P}_X}[X]$已知，我们可以得到一个无偏的方差估计，其定义为
\[
\hat{\sigma}^2_N = \frac{1}{N} \sum_{j = 1}^{N} (x_j - \mu)^2 = \frac{1}{N} \sum_{j = 1}^{N} y_j,
\]
其中$y_j = (x_j - \mu)^2, \quad j=1,\ldots,N$，都是新随机变量$Y = (X - \mathbb{E}_{\mathbb{P}_X}[X])^2$的观测值。实际上，对于$N$个观测值$y_1, y_2, \ldots, y_N$，其经验均值为$\hat{\sigma}^2_N$，而真实均值$\mathbb{E}_{\mathbb{P}_Y}[Y]$为
\[
\mathbb{E}_{\mathbb{P}_Y}[Y] = \mathbb{E}_{\mathbb{P}_X}\left[(X - \mathbb{E}_{\mathbb{P}_X}[X])^2\right] = \sigma^2.
\]

我们对$Y$再次应用切比雪夫定理。则对于任意正的$\delta$，有[14]：
\[
\mathbb{P}_N(|\hat{\sigma}^2_N - \sigma^2| \geq \delta) \leq \frac{\mathrm{var}_{\mathbb{P}_Y}[Y]}{N \cdot \delta^2},
\]
其中$\mathrm{var}_{\mathbb{P}_Y}[Y]$表示$Y$的方差。对于有限的$y_j = (x_j - \mu)^2$，其最小可能取值为$y_{\mathrm{min}} = 0$（当$x_j = \mu$时），最大可能取值为$y_{\mathrm{max}} = (b-a)^2$（当$x_j = a$且$\mu = b$时）。因此，$Y$的取值范围为$R_Y = y_{\mathrm{max}} - y_{\mathrm{min}} = (b-a)^2$。据此可得$Y$的方差上界为
\[
\mathrm{var}_{\mathrm{max}}[Y] \leq R^2_Y/4 = (b-a)^4/4 = R^4_X/4.
\]
因此，可以得到如下结论[14]：
$\mathbb{P}_N\left(|\hat{\sigma}^2_N - \sigma^2| \geq \delta\right) \leq \frac{R^4_X}{4N\delta^2}$。

我们将上述两点总结如下。首先，对于从$\mathbb{P}^N_X$生成的多个$N$元组$(x_1,\ldots,x_N)$，以概率至少$c_1 = 1 - \frac{R^2_X}{4N\epsilon^2}$，有$|\hat{\mu}-\mu|\leq\epsilon$。其次，同样以概率至少$c_2 = 1 - \frac{R^4_X}{4N\delta^2}$，有$|\hat{\sigma}^2_N - \sigma^2| \leq \delta$。我们希望当$\sigma^2$小于$R^4_X / 4$时，使用$\hat{\sigma}^2_N + \delta$作为$\sigma^2$的上界。

然而，我们此前假设我们已知$\mu$。实际上，$\mu$未知。我们需要修正，用$\hat{\mu}$代替$\mu$来计算方差$\hat{\sigma}^2_N$ [14]。此时有$\hat{\hat{\sigma}}^2_N = \frac{1}{N} \sum_{j=1}^{N}(x_j-\hat{\mu}_N)^2$。

我们知道$|\hat{\mu}-\mu| \leq \epsilon$的概率至少为$c_1 = 1 - \frac{R^2_X}{4N\epsilon^2}$。在对$\sigma^2$上界进行估算时的最坏情况，是$\hat{\hat{\sigma}}^2_N$低估了$\hat{\sigma}^2_N$，因为$\hat{\mu}_N$比$\mu$更接近$x_j$。这种情况有两种，我们给出对应的修正：(a) 对于$x_j < \hat{\mu}_N < \mu$，我们计算$(\hat{\mu}_N + \epsilon - x_j)^2$；(b) 对于$\mu < \hat{\mu}_N < x_j$，计算$(x_j - \hat{\mu}_N - \epsilon)^2$。注意这两种情形中，由于都是平方项，因此可以统一写为$(x_j - \hat{\mu}_N - \epsilon)^2 = (x_j - (\hat{\mu}_N + \epsilon))^2$。然后我们有$(x_j - \hat{\mu}_N - \epsilon)^2 \geq (x_j - \mu)^2$，此结论以概率$c_1 = 1 - \frac{R^2_X}{4N\epsilon^2}$成立，针对从$\mathbb{P}_X^N$生成的多个$N$元组$(x_1,\ldots,x_N)$。即，以概率$c_1$，我们可以保证$\hat{\sigma}^2_{N,U} = \frac{1}{N}\sum_{j=1}^{N}(x_j - \hat{\mu}_N - \epsilon)^2 \geq \hat{\sigma}^2_N$。

进一步地，以概率$c_2 = 1 - \frac{R^4_X}{4N\delta^2}$，我们有上界$\sigma^2 \leq \hat{\sigma}^2_N + \delta$。

接下来，我们需要将上述两个事件的概率$c_1$和$c_2$简单组合，即假设它们是独立的 [14]。这需要两组$N$元组观测值。第一组$x_{1,1}, x_{1,2}, \ldots, x_{1,N}$用于估计$\hat{\mu}_{1,N} = \frac{1}{N} \sum_{j=1}^{N} x_{1,j}$。第二组$x_{2,1}, x_{2,2},\ldots,x_{2,N}$用于估计$\hat{\mu}_{2,N} = \frac{1}{N} \sum_{j=1}^{N} x_{2,j}$和$\hat{\sigma}^2_{N,U} = \frac{1}{N} \sum_{j=1}^{N} (x_{2,j} - \hat{\mu}_{2,N} - \epsilon)^2$。于是，以概率$c_1 \cdot c_2 = \left(1 - \frac{R^2_X}{4 N \epsilon^2}\right) \cdot \left(1 - \frac{R^4_X}{4 N \delta^2}\right)$，即
\[
c_1 \cdot c_2 = \left(1 - \frac{R^2_X}{4N\epsilon^2}\right) \cdot \left(1 - \frac{R^4_X}{4N\delta^2}\right)
\]
成立。
⎞·⎛1 − R^4_X N · δ^2⎞，我们有 $\sigma^2 \leq \hat{\sigma}^2_{N,U} + \delta$，考虑到 $\sigma^2 \leq \hat{\sigma}^2_N + \delta$ 且 $\hat{\sigma}^2_N \leq \hat{\sigma}^2_{N,U}$。因此，可以声称以下不等式以至少 $c_1c_2$ 的概率成立：
$${\mathbb{P}}_N(|\hat{\mu}_{1,N} - \mu| \geq \epsilon) \leq \frac{\hat{\sigma}^2_{N,U} + \delta}{N \cdot \epsilon^2}.$$
即
$$\mathbb{P}_N(|\hat{\mu}_{1,N} - \mu| \geq \epsilon) \leq \frac{\hat{\sigma}^2_{N,U} + \delta}{N \cdot \epsilon^2}.$$
为了使该不等式有意义，我们需要 $\sigma^2 \leq \hat{\sigma}^2_{N,U} + \delta < \sigma^2_{\mathrm{max}} = R_X^2 / 4$。我们上述的理论推导被总结为以下引理12.2的证明思路，该引理针对基于方差二次估计的竞争协同进化泛化性能改进统计估计量[14]。

引理12.2（[14]） 对于策略 $i$，考虑两个互相独立且不重叠的包含$N$的测试策略集合：$T_1$ 和 $T_2$，其中 $T_1 \cap T_2 = \emptyset$ 且 $|T_1| = |T_2| = N$。第一组用于估计泛化性能
$$\hat{G}_i(T_1) = \frac{1}{N} \sum_{j \in T_1} G_i(j) 。$$
第二组用于估计方差
$$\hat{\sigma}^2_{N,U} = \frac{1}{N} \sum_{j \in T_2} \big( G_i(j) - \hat{G}_i(T_2) - \epsilon \big)^2,$$
其中 $\epsilon > 0$，且
$$\hat{G}_i(T_2) = \frac{1}{N} \sum_{j \in T_2} G_i(j) 。$$
则对于任意 $\delta > 0$，以概率至少 $c_1c_2 = \left( 1 - \frac{R^2}{4N\epsilon^2} \right) \left( 1 - \frac{R^4}{4N\delta^2} \right)$，下式成立：
$${\mathbb{P}}_N(|\hat{G}_i(T_1) - G_i| \geq \epsilon) \leq \frac{\hat{\sigma}^2_{N,U} + \delta}{N \cdot \epsilon^2}.$$
即
$$\mathbb{P}_N(|\hat{G}_i(T_1) - G_i| \geq \epsilon) \leq \frac{\hat{\sigma}^2_{N,U} + \delta}{N \cdot \epsilon^2}。$$
(12.13)

12.3.2 高斯分布泛化性能估计误差分析

虽然相比假设最坏情况的切比雪夫界限，我们现在能够获得更紧的界，但这需要两倍的计算成本，因为需要第二个不重叠的随机测试策略组。这里，我们将直接讨论提升泛化性能估计的手段，并利用泛化性能估计的一个潜在性质，即当游戏结果的样本量足够大时，其均值近似服从高斯分布。我们在后续的正式分析中[13]，采用了游戏结果对随机测试策略的高阶（第二阶和第三阶）矩来估计平均游戏结果（即泛化性能）分布收敛到标准高斯分布的速度（以样本量为标准）。具体来说，测试策略样本 $S_N$ 的选择可以形式化为概率空间 $(S^N, \mathbb{P}_{\mathcal{S}})$ 上的随机变量。对于策略 $i$ 的泛化性能估计，可被看作是随机变量 $\hat{G}_i(\mathcal{S}_N)$ 的一组实现。由于游戏结果 $G_i(J)$ 具有有限的均值和方差，根据中心极限定理，当 $N$ 充分大时，......
$\hat{G}_i(\mathcal{S}_N)$服从高斯分布。关于$\hat{G}_i(\mathcal{S}_N)$的（累积分布）收敛到高斯（累积分布）的速度，可以利用Berry-Esseen定理[44]进行定量描述。为了形式化证明这一点，我们需要将$\hat{G}_i(\mathcal{S}_N)$归一化为均值为零、标准差为一[13]。首先，将$G_i(J)$归一化为均值为零：$X_i(J) = G_i(J) - G_i$。

记$G_i(J)$（因此$X_i(J)$）的方差为$\sigma_i^2$。由于$G_i(J)$取自有限域，$X_i(J)$的三阶绝对矩$\rho_i = \mathbb{E}_{\mathbb{P}_{\mathcal{S}}}[|X_i(J)|^3]$是有限的。其次，将$\hat{G}_i(\mathcal{S}_N)$归一化为均值为零：$Y_i(\mathcal{S}_N) = \hat{G}_i(\mathcal{S}_N) - G_i = \frac{1}{N} \sum_{j \in \mathcal{S}_N} X_i(j)$。

第三，将$\hat{G}_i(\mathcal{S}_N)$归一化为单位标准差：$Z_i(\mathcal{S}_N) = \frac{Y_i(\mathcal{S}_N)}{\frac{\sigma_i}{\sqrt{N}}}$。

Berry-Esseen定理表明，$Z_i(\mathcal{S}_N)$的累积分布函数（cdf）$F_i$逐点收敛于标准正态分布$N(0, 1)$的累积分布函数$\varPhi$：对于任意$x \in \mathbb{R}$，

$$
|F_i(x) - \varPhi(x)| \le \frac{0.7975}{\sqrt{N}} \frac{\rho_i}{\sigma_i^3}
$$
(12.14)

仅需$\sigma_i$与$\rho_i$即可对$Z_i(\mathcal{S}_N)$与$N(0,1)$的cdf逐点差值进行估计。由于实际中$\sigma_i$与$\rho_i$的理论矩未知，因此通常使用它们的经验估计值。为了保证$Z_i(\mathcal{S}_N)$与$N(0,1)$的cdf逐点差值不超过$\epsilon_{\mathrm{cdf}} > 0$，至少需要$N_{\mathrm{cdf}}(\epsilon)$个测试策略，其计算方法如下[13]：

$$
N_{\mathrm{cdf}}(\epsilon) = \frac{0.7975^2}{\epsilon_{\mathrm{cdf}}^2} \frac{\rho_i^2}{(\sigma_i^2)^3}
$$
(12.15)

接下来我们展示通过参数检验方式，新的统计估计量如何有望通过降低样本量$N$，从而降低计算成本，相较于先前的估计器取得改进[13]。假设泛化估计服从高斯分布（如上述分析所示，我们采集了足够多的测试点，使得其均值近似服从高斯分布）。记$z_{\alpha/2}$为标准正态分布$N(0,1)$的上侧$\alpha/2$分位点，即在区间$(z_{\alpha/2}, \infty)$下的标准正态分布密度面积为$\alpha/2$，而在$[-z_{\alpha/2}, z_{\alpha/2}]$内面积为$1-\alpha$。对于大样本的策略集$\mathcal{S}_N$，其泛化性能$\hat{G}_i(\mathcal{S}_N)$的标准误为$\frac{\sigma_i}{\sqrt{N}}$。
$N \frac{\sigma_i}{\sqrt{N}}$，而$\hat{G}_i(S_N)$的$100(1-\alpha)\%$误差界限为$z_{\alpha/2} \frac{\sigma_i}{\sqrt{N}}$。由于$\sigma_i$通常是未知的，标准误差可以被估计为 [13]

\[
\frac{\hat \sigma_i(S_N)}{\sqrt{N}} = \sqrt{ \frac{\sum_{j \in S_N} \big(G_i(j) - \hat{G}_i(S_N)\big)^2}{N(N-1)} }
\]

以给出$\hat{G}_i(S_N)$的$100(1-\alpha)\%$误差界限$\varUpsilon_i(\alpha,N)$，其计算公式为

\[
\varUpsilon_i(\alpha,N) = z_{\alpha/2} \cdot \sqrt{ \frac{\sum_{j \in S_N} \big(G_i(j) - \hat{G}_i(S_N)\big)^2}{N(N-1)} }
\]

这使得我们可以构造如下表达式：当要求误差界限最多为$\delta_{\mathrm{em}} > 0$时，所需样本量至少为 [13]：

\[
N_{\mathrm{em}}(\delta) = \frac{z_{\alpha/2}^2 \sigma_i^2}{\delta_{\mathrm{em}}^2}
\]

换句话说，我们需要$N_{\mathrm{em}}(\delta)$个测试策略，以$100(1-\alpha)\%$的把握确保$|\hat{G}_i(S_N) - G_i| < \delta_{\mathrm{em}}$。此时，策略$i$的真实泛化性能的$100(1-\alpha)\%$置信区间为

\[
(\hat{G}_i(S_N) - \varUpsilon_i(\alpha,N),\ \hat{G}_i(S_N) + \varUpsilon_i(\alpha,N))
\]

另外，关于泛化性能估计的高斯分布特性，我们还可以加以利用。特别地，我们用中心极限定理的推理，同样适用于实际中真实值未知时对$\sigma_i$的样本估计 [13]。对于$m$个独立同分布的测试策略样本$S^r_N, \ r = 1,2, \ldots, m$，每个样本规模为$N$，其泛化性能估计为

\[
\hat{G}_i(S^r_N) = \frac{1}{N} \sum_{j \in S^r_N} G_i(j)
\]

当$N$足够大时，这些估计近似服从均值为$G_i$、标准差为$\sigma_i/\sqrt{N}$的高斯分布。这些泛化性能估计值可用于估算$\sigma_i$的置信区间。

这些估计$\hat{G}_i(S^r_N)$的样本方差$V^2_m$为 [13]
$$
V^2_m = \frac{\sum_{r=1}^m (\hat{G}_i(S^r_N) - \varGamma_i)^2}{m-1}, \\
\varGamma_i = \frac{1}{m}\sum_{r=1}^m \hat{G}_i(S^r_N).
$$

关键的是，高斯分布的 $\hat{G}_i(S^r_N)$ 的归一化样本方差 $U^2_m$，即
$$
U^2_m = \frac{(m-1) V^2_m}{\frac{\sigma_i^2}{N}}
$$
服从自由度为 $m-1$ 的 $\chi^2$ 分布。需要注意的是，关于第三阶矩存在一个平凡的不等式（见文献 [17] 第210页），但其导出的界限较为宽泛。因此，$\sigma_i/\sqrt{N}$ 的 $100(1-\alpha)\%$ 置信区间为
$$
\left( V_m \cdot \sqrt{\frac{m - 1}{\chi^2_{\alpha/2}}} \ , \ V_m \cdot \sqrt{\frac{m - 1}{\chi^2_{1-\alpha/2}}} \right),
$$
其中，$\chi^2_{\beta}$ 表示在 $N-1$ 自由度的 $\chi^2$ 分布下，右侧面积为 $\beta$ 的取值。进一步地，我们可以得出 $\sigma_i$ 的 $100(1-\alpha)\%$ 置信区间为
$$
\left( V_m \cdot \sqrt{\frac{N(m - 1)}{\chi^2_{\alpha/2}}} \ , \ V_m \cdot \sqrt{\frac{N(m - 1)}{\chi^2_{1-\alpha/2}}} \ \right)
$$
我们将其重写为便于直接计算的形式 [13]：
$$
\left( \sqrt{\frac{N \cdot \sum_{r=1}^m (\hat{G}_i(S^r_N) - \varGamma_i)^2}{\chi^2_{\alpha/2}}} \ , \ \sqrt{\frac{N \cdot \sum_{r=1}^m (\hat{G}_i(S^r_N) - \varGamma_i)^2}{\chi^2_{1-\alpha/2}}} \ \right).
$$

(12.19) 

### 12.3.3 策略比较的统计检验

已知 $\hat{G}_i(S_N)$ 服从高斯分布，这使我们能够针对策略的比较进行统计显著性检验 [13]。首先，研究者往往关心哪一类策略能达到比某一性能阈值 $\tilde{G}$ 更好的泛化表现。我们可以检验策略 $i$ 是否为低性能策略，即检验 $G_i < \tilde{G}$。若假设 $H_1$：$G_i < \tilde{G}$ 在显著性水平为 $\alpha\%$ 下（与原假设 $H_0$：$G_i = \tilde{G}$ 对比）得以成立，则需要满足如下检验统计量：
$Z^{\prime}_i(S_N, \tilde{G}) = \frac{\hat{G}_i(S_N) - \tilde{G}}{\sqrt{ \frac{\sum_{j \in S_N} (G_i(j) - \hat{G}_i(S_N))^2}{N(N-1)}}}$

当 $Z^{\prime}_i(S_N, \tilde{G})$ 低于 $-z_\alpha$，即 $Z^{\prime}_i(S_N, \tilde{G}) \le -z_\alpha$ 时，拒绝原假设。或者说，若 $Z^{\prime}_i(S_N, \tilde{G}) > z_\alpha$，则可以在 $\alpha\%$ 置信水平下接受策略 $i$ 是可接受策略的假设，即 $G_i > \tilde{G}$（相对于 $H_0$）。我们还可以进行无方向性检验（nondirectional test）：判断 $G_i \neq \tilde{G}$，此时需要满足 $|Z^{\prime}_i(S_N, \tilde{G})| > z_{\alpha/2}$。

关键的是，我们可以对集合 $\mathcal{S}$ 中的两个策略 $i, j \in \mathcal{S}$ 的相对性能进行比较 [13]。在（协）进化学习环境下，这对于新一代策略的构建十分重要。设两策略 $i$ 和 $j$ 均与同一组 $N$ 个测试策略 $S_N = \{ t_1, t_2, \ldots , t_N \}$ 对抗。针对 $i$ 和 $j$ 的真实泛化性能之间的关系，可以通过配对检验（paired tests）进行统计测试。具体地，计算 $S_N$ 上的一系列性能差异：

$$
D(n) = G_i(t_n) - G_j(t_n), \quad n = 1, 2, \ldots, N.
$$

这些性能差异作为一个单一样本进行分析。在显著性水平 $\alpha\%$ 下，若 $Z^{\prime\prime}_i(S_N, \tilde{D}) \ge z_\alpha$，则认为策略 $i$ 比策略 $j$ 优于一个阈值 $\tilde{D}$（与原假设“$i$ 比 $j$ 恰好优于 $\tilde{D}$”进行对比），其中

\[
Z^{\prime\prime}_i(S_N, \tilde{D}) = \frac{\hat{D}(S_N) - \tilde{D}}{\sqrt{ \frac{\sum_{n=1}^N (D(n) - \hat{D}(S_N))^2}{N(N-1)}}}, \\
\hat{D}(S_N) = \frac{1}{N} \sum_{n=1}^N D(n).
\]

仅仅测试策略 $i$ 是否优于策略 $j$，我们只需令 $\tilde{D} = 0$。同样地，若 $Z^{\prime\prime}_i(S_N, 0) \le -z_\alpha$，则说明在显著性水平 $\alpha\%$ 下，策略 $i$ 的性能低于策略 $j$。最后，若 $|Z^{\prime\prime}_i(S_N, 0)| \ge z_{\alpha/2}$，则可以认为在置信水平 $\alpha\%$ 下，策略 $i$ 与策略 $j$ 存在显著差异 [13]。

需要特别注意的是，策略 $i, j \in \mathcal{S}$ 的统计比较，是借助于一组测试策略 $S_N$ 实现的。这与仅仅通过双方直接对局的两两比较截然不同。如果比较方式选取不当，博弈中的非传递性（intransitivity）可能导致误导性的结果。例如，有人可能会声称两个策略“在锦标赛图的强连通子集内等价”（参见第 5.3.1 节），这时，策略 $j$ 获得更高的分数，意味着其相较于 $i$ 有更高的泛化性能。然而，在直接对局中，$i$ 仍然可能击败 $j$。

12.3.4 协同进化学习中改进泛化性能估计的计算研究
在此，我们参考文献[13, 14]中几个计算实验，以展示泛化性能估计如何得到改进并加以应用。首先，我们证明了在实际中，Chebyshev界限在假设最坏情况下 $\sigma^2_{\mathrm{max}} = R^2/4$ 时，可以通过对 $\sigma^2$ 的再次估计进行改进，前提是假设 $\hat{\sigma}^2_{N,U} + \delta < \sigma^2_{\mathrm{max}}$。需要注意的是，为了以高概率 $c_1 c_2$ 应用由等式(12.13)给出的更紧致的Chebyshev界限，其中 $c_1 = 1 - R^2/(4N\epsilon^2)$，$c_2 = 1 - R^4/(4N\delta^2)$，我们希望 $c_2$ 尽可能高，因为这正是该更紧致界限有用的情形，即对于策略 $i$，$\sigma^2$ 较小，并且即使对较大的 $\delta$，依然有 $\hat{\sigma}^2_{N,U} + \delta < \sigma^2_{\mathrm{max}}$。我们的计算实验再次应用了Chebyshev定理，这一次关注随机变量 $|\hat{\sigma}^2_N - \sigma^2|$，以经验地考察在三选IPD博弈中，用一组测试策略样本估算 $\sigma^2$ 的精度，因为我们实际上可以计算出 $\sigma^2$ 的真实值：

$${\mathbb{P}}_N(|\hat{\sigma}^2_N - \sigma^2| \geq \delta) \leq \frac{R^4}{4 N \cdot \delta^2}.$$

我们可以通过令 $|D(\hat{\sigma}^2_N)|' = |\hat{\sigma}^2_N - \sigma^2| / R^2$ 和 $\delta' = \delta / R^2$ 对上述公式进行简化，因此得到：

$${\mathbb{P}}_N(|D(\hat{\sigma}^2_N)|' \geq \delta') \leq \frac{1}{4 N \cdot (\delta')^2}.$$

实验针对 $G_{\mathrm{W}}(i)$ 进行，检验4000个随机策略中，$|D(\hat{\sigma}^2_N)|' > \delta'$ 的经验概率（比例），其中50组样本 $S_N$ 的规模 $N \in \{10, 50, 100, 500, 1000, 5000, 10000\}$ 均独立均匀采样。需要指出的是，我们实际上计算的是 $\hat{\hat{\sigma}}^2_N$ 而非 $\hat{\sigma}^2_N$。不过，实验结果显示，大多数情况下 $\hat{\hat{\sigma}}^2_N$ 会高估 $\sigma^2$。更重要的是，可以构造修正的估计量 $\hat{\sigma}^2_{N,U}$，并据此获得更紧致的Chebyshev界。

表12.1给出了不同 $N$ 下的 $\hat{\sigma}^2_N$ 及若干 $G_{\mathrm{W}}(i)$ 随机策略的实际 $\sigma^2$ 真值；由表中数据可得 $|\hat{\sigma}^2_N - \sigma^2|$ 的结果。如采用较大的精度阈值 $\delta' = 0.04$（即 $\delta = 400$），例如 $N = 10000$ 时，根据Chebyshev界知 $|\hat{\sigma}^2_N - \sigma^2| \leq \delta$ 的概率为 $1 - 1/(4\times 10000 \times 0.04^2) > 0.98$。然而，实际对 $\sigma^2$ 的估计精度通常要优于理论界所示。例如，对于 $N=10000$，以概率大于 $0.98$ 时，$|\hat{\sigma}^2_N - \sigma^2|$ 远小于400。需要注意的是，表12.1中同一策略的 $\hat{\sigma}^2_N$ 会在 $\sigma^2$ 真值附近波动（可高可低）。极小样本（例如 $N=10$，$\hat{\sigma}^2_N=0$）情况下可能出现极端结果。我们的结果并未显示 $|D_N|$ 随 $N$ 单调递减。表12.1对比了 $G_{\mathrm{W}}(i)$ 的十个策略在不同 $N$ 下的 $\hat{\sigma}^2_N$ 与真实值 $\sigma^2$，其中 $\sigma^2 \in [0, 2500]$ [14]。
N值的增加是由于所计算的$\sigma^2$优于Chebyshev界所给出的结果[14]。如果$\hat{\sigma}^2_N + \delta < \sigma^2_{\mathrm{max}}$，则可以得到一个更紧的Chebyshev界。对于特定策略（表12.1中的#7），有$\sigma^2 = 1293$，而$\hat{\sigma}^2_N = 1321$。修正后的估计为$\hat{\sigma}^2_{N,U} = 1337$（其中$\hat{\sigma}^2_{N,U} \geq \hat{\sigma}^2_N$的概率为$c_1 = 1 - 1/(4 \times 10000 \times 0.04^2) > 0.98$）。在这种情况下，我们知道$\hat{\sigma}^2_N$高估了$\sigma^2$。无论如何，以概率$c_1c_2 = (1 - 1/(4 \times 10000 \times 0.04^2)) \times (1 - 1/(4 \times 10000 \times 0.04^2)) > (0.98) \times (0.98) > 0.95$，我们可以知道$\sigma^2 \leq \hat{\sigma}^2_{N,U} + \delta$，其中$\delta = 400$。在此，我们得到$\hat{\sigma}^2_{N,U} + \delta = 1337 + 400 < \sigma^2_{\mathrm{max}} = 2500$[14]。关于$\sigma^2$的估计优于Chebyshev界的这个事实，自然引出了一个问题：是否可以可靠地利用小样本量估计泛化性能。事实上，我们已在第12.3.4节中对此进行了理论解答。我们无需对博弈如何影响策略$i \in \mathcal{S}$对一组随机样本$S_N$的结果分布进行详细考察。尽管如此，我们的方法论利用了泛化估计近似高斯分布的特性，并确定了随着随机测试策略样本$S_N$的规模$N$增加，泛化估计的分布收敛于高斯分布的速度。在下面的计算实验中[13]，我们采用三选IPD博弈以说明我们的方法。我们首先随机抽取50个基础策略$i \in \mathcal{S}$。同时，我们抽取1000个独立样本$S_N$以计算1000个估计$\hat{G}_i(S_N)$。每个随机样本$S_N$由$N$个从$\mathcal{S}$中独立同分布（iid）抽取的测试策略组成。在所有抽样情况下，我们采用均匀分布$\mathbb{P}_{\mathcal{S}}$。对于该博弈，我们直接计算$G_i$，以便获得$X_i(J) = G_i(J) - G_i$。接着，以$S_N$对$X_i(J)$分别计算方差和第三阶绝对矩的估计量，即得到$\hat{\sigma}_i^2$的1000个样本估计，以及$\hat{\rho}_i$的另1000个样本估计。根据Berry-Esseen定理（式(12.15)）[44]，我们可以为每个基础策略$i$计算和高斯分布的偏离量$\epsilon_{\mathrm{cdf}}$：
$$\epsilon_{\mathrm{cdf}} = \frac{0.7975 \cdot \hat{\rho}_i}{\sqrt{N} \cdot \hat{\sigma}_i^3}$$
我们针对不同的样本规模 $S_N$，给出了 $N \cdot \hat{\sigma}^3_i$（公式12.23）的统计结果。通过系统地计算 $N = \{50, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 10000, 50000\}$ 时的误差 $\epsilon_{\mathrm{cdf}}$，可以观察到泛化估计量分布收敛到高斯分布的速度。由于 $\epsilon_{\mathrm{cdf}}$ 的真实值未知，我们采用了一种保守（悲观）的估计方法。首先，将1000次采样得到的 $\hat{\sigma}^2_i$ 和 $\hat{\rho}_i$ 分别按升序排序。$\epsilon_{\mathrm{cdf}}$ 的保守估计是取 $\hat{\sigma}^2_i$ 的较小值（2.5%分位数）和 $\hat{\rho}_i$ 的较大值（97.5%分位数）。虽然对于 $\hat{\sigma}^2_i$ 可以直接由 $\chi^2$ 分布计算分位区间，但对于 $\hat{\rho}_i$ 仅能依据文献[17]第210页的不等式给出较宽松的界限，这会导致 $\epsilon_{\mathrm{cdf}}$ 的保守估计值不必要地偏大。

我们将利用经验分布和由 $\chi^2$ 分布（公式12.19）直接得到的估计，对于1000次采样下 $\hat{\sigma}^2_i$ 的分位点进行对比。结果显示，当 $N=50$（我们考虑的最小样本量）时，绝对误差约为 $0.03$，且当 $N$ 变大时平均误差更小。因此，后续实验中我们可以直接采用通过经验方法获得的 $\hat{\sigma}^2_i$ 和 $\hat{\rho}_i$ 的分位数。需要注意的是，依据Glivenko-Cantelli定理[17, 51]，基于样本值得到的经验分布函数能够一致逼近真实分布函数。

表12.2列出了十种策略 $i$ 下，不同 $N$ 值对应的 $\epsilon_{\mathrm{cdf}}$ 的保守估计。结果表明，随着 $N$ 的增加，$\epsilon_{\mathrm{cdf}}$ 不断减小，但泛化性能更鲁棒估计和计算开销也存在权衡。特别地，结果显示在 $N$ 从50增加到1000时，$\epsilon_{\mathrm{cdf}}$ 降低很快，但在 $N=1000$ 左右开始趋于平稳[13]。表12.2提示，在 $N=2000$ 时，$S_N$ 可为泛化性能提供足够鲁棒的估计，同时计算开销仍在合理范围内，因为如果想将 $\epsilon_{\mathrm{cdf}}$ 减半，则需要将 $N$ 增加5倍达到10000。要注意的是，对于小样本（如策略7在 $N=50$ 时），$\epsilon_{\mathrm{cdf}}$ 的保守估计可能会大于真实值，但可通过直接取最大值1加以修正。此外，公式（12.23）中的 $\hat{\sigma}^2_i$ 和 $\hat{\rho}_i$ 均为有界的有限矩。即使是非常规策略（例如在与多种对手测试策略对弈时表现出高度波动），对于较大的 $N$，$\epsilon_{\mathrm{cdf}}$ 仍主要受 $N^{-1/2}$ 项支配。这表明，绝大多数策略在泛化性能估计鲁棒性与计算开销之间的权衡大致相同[13]。

暂时假定基准策略 $i$ 下的 $\hat{G}_i(\mathcal{S}_N)$ 近似服从高斯分布。我们可利用公式（12.17）得到如下误差：

$$
\delta_{\mathrm{Gauss}} = \frac{z_{\alpha/2} \ \hat{\sigma}_i}{\sqrt{N}}
$$

表12.2 十种策略 $i$ 在公式（12.23）下 $\epsilon_{\mathrm{cdf}}$ 的保守估计。结果展示了泛化性能更鲁棒估计与计算开销增加之间的权衡；增加样本数量 $N$ 能减少误差 $\epsilon_{\mathrm{cdf}}$，但该效应在 $N=1000$ 左右开始趋于平稳[13]。
表12.3 50种策略$i$的${|\epsilon_{\mathrm{cdf}} - \delta_{\mathrm{Gauss}}|}_i$统计数据[13]
我们计算了$\delta_{\mathrm{Gauss}}$的悲观估计值，取自经过排序的1000次采样估计中的$\hat{\sigma}_i^2$的97.5百分位数。我们的$\delta_{\mathrm{Gauss}}$结果同样表明在更为稳健的估计和计算开销之间存在权衡，并且建议在$N = 2000$时，$S_N$能够以合理的计算代价提供足够稳健的泛化性能估计。表12.3的结果显示，样本容量越大，$\{|\epsilon_{\mathrm{cdf}} - \delta_{\mathrm{Gauss}}|\}_i$的值越小，例如当$N > 1000$时，绝对差值小于0.01，并且同样表明了类似的权衡[13]。我们验证了在协同进化学习中泛化性能的统计估计量是如何被改进的。现在，我们展示如何将这样改进的统计估计量应用于更复杂的奥赛罗（Othello）棋游戏的协同进化学习[13]。

奥赛罗是一种确定性、完全信息、零和的棋盘游戏，由两名选手（黑方与白方）轮流在8x8棋盘上下子（相同颜色）。如图12.7所示，对局以每个选手棋盘上已有两枚棋子开始。黑方首先落子。一次合法走法指的是：新下的棋子必须水平、垂直或对角线与对方已有棋子相邻，并且使得在玩家的新棋子与已有棋子之间至少有一枚对方棋子能够被翻转成自己的棋子，从而完成该步（例如，见图12.7右）。若玩家无法进行合法走子，则让子权移交给对方。若棋盘已满，或双方连续均无法落子，则对局结束[11]。

在奥赛罗协同进化学习中，策略的表示方式（一般以棋盘评价函数的形式，然后输入到博弈树的极小极大搜索中）有加权棋子计数器（WPCs）[43]和人工神经网络（ANNs）[11, 42]。出于表示简单、计算成本低，以及可论证每个棋子计数有限精度即可的原因，我们主要使用WPCs。

图12.7展示了奥赛罗棋的基本走法。（左）为比赛开始时双方棋子的初始位置。（右）为黑方在游戏某一阶段可能的合法走法（以灰色圆圈标出）。黑方选中并执行一个合法落子后，被包围的白棋会被翻转为黑棋，从而完成该步。
WPC中的元素意味着WPC表达方式会诱导出一个非常庞大但有限的策略空间$\mathcal{S}$，因此后者在度量泛化性能时需要使用统计估算方法。用于描述Othello（黑白棋）博弈策略的棋盘评估函数的WPC，可表示为一个64维的权重向量，索引为$w_{rc}$，其中$r=1,\ldots,8$，$c=1,\ldots,8$，$w_{rc} \in[-10,10]$，这里$r$与$c$分别代表8×8黑白棋棋盘的行和列的索引。对应地，Othello棋盘状态可表示为$x_{rc}$，其中$r=1,\ldots,8$，$c=1,\ldots,8$，$x_{rc} \in \{-1,0,+1\}$，其中$+1$、$-1$和$0$分别表示黑子、白子和空格[43]。则对于每一棋盘状态$\boldsymbol{x}$，WPC的输出计算方式为：
\[
\mathrm{WPC}(\boldsymbol{x}) = \sum_{r=1}^8 \sum_{c=1}^8 w_{rc} \cdot x_{rc},
\]
即
\[
\mathrm{WPC}(\boldsymbol{x}) = \sum_{r=1}^8 \sum_{c=1}^8 w_{rc} \cdot x_{rc}, \tag{12.24}
\]
其中，$\mathrm{WPC}(\boldsymbol{x})$的值越正（负）表示，如果WPC为黑（白）棋方，则其认为棋盘状态$\boldsymbol{x}$越有利[43]。

我们采用一种简单的加性变异算子，通过如下方式根据父代$w_{rc}$生成子代的WPC $w'_{rc}$：
\[
w'_{rc} = w_{rc} + k \cdot U_{rc}(-1,1), \quad r=1,\ldots,8,\ c=1,\ldots,8,
\]
即
\[
w'_{rc} = w_{rc} + k \cdot U_{rc}(-1,1),\ r=1,\ldots,8,\ c=1,\ldots,8, \tag{12.25}
\]
其中$k$为缩放常数（$k=0.1$），$U_{rc}(-1,1)$是在$[-1, 1]$区间内均匀分布的随机实数，并且对于每个$r$和$c$组合（共64组权重）都重新采样。该简单的变异算子能够为WPC评估函数形式表示的Othello游戏策略提供足够的多样性。策略表示及变异算子中各种参数的设定并未进行优化。我们后续计算实验的重点在于研究作为适应度度量的泛化性能估计对协同进化学习泛化性能提升的影响。
由于黑白棋（Othello）游戏并不一定具有对称性 [54]，我们考虑了两种泛化性能估计方法：分别用黑方（白方）加权位置棋（WPC）与随机采样的白方（黑方）WPC进行对弈测试。鉴于黑白棋是一个零和游戏，我们采用$\hat{G}_{\mathrm{W}}(S_N)$ 作为评估指标。考虑到执行一局黑白棋的计算开销远高于IPD（囚徒困境）游戏，我们采用了50000个测试WPC的随机样本进行评估，其中每一对$r$和$c$的权值$w_{rc}$根据均匀分布$w_{rc} \sim U_{rc}[-10,10]$重新采样，并取对方颜色，用以估计协同进化WPC的泛化性能。我们再次使用算法12.2中描述的CCL方法，其过程调用为$\mathrm{CCL}(\mathbf{Y}, b_{\mathrm{gen}})$，其中采用完整的种群$2\lambda$，并将$b_{\mathrm{gen}}=200$代作为基线。我们将CCL的泛化性能与直接采用$\hat{G}_i(S_N)$作为适应度评估指标的协同进化学习进行比较，后者称为改进型协同进化学习（ICL）；此外，还与混合采用$\hat{G}_i(S_N)$与循环赛得分作为适应度评估标准的CCL-ICL混合方法进行了比较。特别地，在IPD游戏的协同进化实验结果显示，混合方法相较于ICL没有显著差异，但两者在提升泛化性能方面均优于CCL [13]。因此，以下将仅比较CCL和ICL。所有实验均重复两次：一次针对黑方WPC，一次针对白方WPC。每组实验均独立重复30次以便统计分析。CCL使用$2\lambda=50$，ICL使用$2\lambda=20$，同时ICL采用不同样本规模$N=\{50, 500, 1000, 5000, 10000, 50000\}$的$S_N$来估计$\hat{G}_i(S_N)$。对于$N=50000$的样本$S_N$，用以获得接近泛化性能真实值的判断基础，从而使得更小样本下的结果有比较参照。需要注意的是，在ICL中，不同的样本$S_N$用于分别计算$\hat{G}_i(S_N)$作为适应度指标和用于评估泛化性能分析。图12.8汇总了我们的实验结果，每一幅图均绘制了进化过程中种群中性能最优策略于全部30次独立运行中的$\hat{G}_i(S_N)(N=50000)$结果。黑方和白方WPC的CCL结果显示，协同进化过程中策略的泛化性能波动较大（图12.8a与b）[13]。然而我们观察到，尽管在协同进化过程中，当使用50个测试策略的小样本估算$\hat{G}_i(S_N)$时会出现小幅波动，ICL仍能搜索到泛化性能更高的WPC。随着样本规模的提升，ICL的泛化性能也进一步提升，比如在$N=500$时（图12.8c与d）。尽管如此，样本规模的显著增加却不能带来ICL泛化性能的显著提升。例如，ICL-N5000与ICL-N50000的结果接近（图12.8e与f）。这一发现与实验中为了获得鲁棒泛化性能估计所需测试策略数量的研究结果一致，即提升效益约在$N=5000$时逐渐转化为收益递减 [13]。
图12.8 CCL与不同ICL在黑白棋游戏中的对比：（a）CCL 黑棋WPC，（b）CCL 白棋WPC，（c）ICL-N500 黑棋WPC，（d）ICL-N500 白棋WPC，（e）ICL-N50000 黑棋WPC，以及（f）ICL-N50000 白棋WPC。图中绘制了在整个进化过程中，种群中表现最优策略的估算泛化性能 $\hat{G_i}(S_N)$（其中 $N = 50000$），所有结果均基于30次独立实验[13]。

12.4 讨论与结论
在本章中，我们从理论上构建了竞争共进化学习的一般化框架，其中个体间的相互作用可被建模为博弈过程。策略（解）的泛化性能通过一组随机测试策略（测试用例）采样，并取平均博弈结果进行估算，同时利用切比雪夫定理（Chebyshev’s Theorem）为其提供置信界。所建立的切比雪夫置信界的优点在于，其对于任何博弈结果分布均适用。然而，分布无关的分析框架往往会导致过于宽松的置信界。我们直接针对这个问题，利用了平均博弈结果近似高斯分布（near-Gaussian）的性质，基于参数化检验（parametric testing）提出了更紧的置信界。这一改进使我们能够构建更优的统计估计器，仅需较小规模的测试策略样本即可对泛化性能进行估算。

随后，我们将改进后的估计量作为适应度度量，应用于更复杂的奥赛罗（Othello）博弈的竞争共进化。结果表明，新方法在竞争共进化学习（CCL）上具有显著提升。该方法加快了共进化搜索过程，并且我们能够严格控制实现加速的条件（样本容量），而不会以降低估计精度为代价，从而促使策略进化出更高的泛化性能。

尽管上述改进提升了竞争共进化的搜索性能，使用估计量 $\hat{G_i}(S_N)$ 作为适应度度量的方法更类似于进化学习方式。更重要的是，这种泛化性能的度量是针对策略空间 $\mathcal{S}$ 上的均匀概率分布 $\mathbb{P}_{\mathcal{S}}$ 而提出的。可以预期，在策略空间 $\mathcal{S}$ 中，绝大多数策略的博弈性能较低。如何直接从策略空间 $\mathcal{S}$ 中针对高性能策略采样以生成测试样本，这一问题仍未解决。这是因为，即使对于简单游戏，目前仍不清楚是否容易构建一个适当的度量空间 $\mathcal{S}$，使得能够进一步定义出倾向于高性能策略的概率分布 $\mathbb{P}_{\mathcal{S}}$。

在大多数情况下，需要对策略空间 $\mathcal{S}$ 中的博弈行为进行深入分析（如针对IPD博弈的自动化策略响应分析与指纹识别技术[1]），从而将其组织为等价类（即适当度量空间 $\mathcal{S}$ 上的局部邻域）。随后，希望这些等价类能按性能递增顺序组织起来，因为相应的博弈具有可解性（即在 $\mathcal{S}$ 中存在一组主导其他策略的子集）。这实际上也涉及到策略表示问题——可以设想实现一种高效的表示方式，将所有等价策略在紧凑的 $\mathcal{S}$ 中归约为单个代表点，并保证这些代表点之间具有可传递链式关系。因此，如何通过启发式方法设计和优化搜索以提升竞争共进化系统的搜索性能，仍具有广阔空间和重大动因。

本章最后，我们聚焦竞争共进化系统中的一个重要研究领域。具体而言，我们此前开展了详尽的计算实验，考察共进化学习中泛化性能与多样性之间的关系[15]。这一直是文献中悬而未决的重要问题，需要深入理解。因为群体缺乏多样性，普遍被认为是导致各种共进化病理现象的原因，进而对共进化搜索性能产生负面影响。我们的计算研究系统考察了采用不同基于启发式的多样性维持机制时，竞争共进化系统的多样性水平和泛化性能。我们选择了四选一囚徒困境（4-choice IPD）博弈作为案例研究场景，因为该场景下我们能够定量测量群体的泛化性能和多样性水平。
对于多样性度量，我们借鉴了之前在文献 [8, 37] 中研究的方法，这些文献对遗传编程（GP）的多种多样性度量进行了系统分析。接下来我们将简要介绍我们所采用的相关方法。

设一个以直接查找表（direct look-up table）形式表示的囚徒困境（IPD）博弈策略被编码为字符串 $\mathbf{s}_i = (i_k : k = 1,\ldots,n_e),\ n_e = n^2 + 1$，其中 $n_e$ 是直接查找表的总元素数，$n$ 是具有离散合作级别的IPD博弈选择数。那么，策略 $i$ 和 $j$ 之间的编辑距离，定义为将策略 $i$ 的直接查找表 $\mathbf{s}_i$ 转换为 $\mathbf{s}_j$ 所需的最小变换次数，具体计算如下：

$$
\mathrm{dist}_{\mathrm{ed}}(\mathbf{s}_i, \mathbf{s}_j) = \frac{1}{n_e}\sum_{k=1}^{n_e} d(i_k, j_k),
$$

其中，$d(i_k, j_k)$ 是 $i_k$ 与 $j_k$ 的距离度量。对于距离度量 $d(i_k, j_k)$，我们采用Levenshtein距离 [47]，即

$$
d(i_k, j_k) =
\begin{cases}
0, & i_k = j_k \\
1, & i_k \neq j_k.
\end{cases}
$$

对于某一特定元素的变换，仅允许在不同选择之间进行替换操作。例如，如果两种策略 $i$ 和 $j$ 仅仅在第一次行动的选择上不同，比如策略 $i$ 选择 $+1$，策略 $j$ 选择 $-1$，则只需要一个变换操作。这两种策略之间的编辑距离为 $1/n_e$。

因此，可以在协同进化种群中基于编辑距离来定义基因型多样性度量，该度量通过取种群中策略两两之间编辑距离的平均值获得 [27]。此外，也可以定义表现型多样性度量，以量化表现型空间中的多样性水平。这类多样性度量对于博弈问题非常有用，因为在这些问题中，基因型与表现型之间可能存在多对一的关系，即许多基因型不同的策略在面对不同对手时可能表现出相似的行为。用于这类度量的方法包括基于指纹（fingerprints）的复杂自动分析 [1]，也有更为简单的表现型多样性度量。我们采用后者，具体来说，使用基于熵的多样性度量方法 [8, 52]。
$-\sum_{k} p_k \cdot \log p_k$，其中$p_k$指的是群体中某一特定适应度值所占的比例。在我们的案例研究中，适应度值是通过让整个共同进化策略群体进行循环赛（round-robin tournament）得到的平均IPD（囚徒困境）博弈得分。我们研究的动机是要回答两个相关问题：（1）哪种形式的多样性有利于泛化， （2）需要多少合适形式的多样性才能获得良好的泛化性能。这样，关于多样性的具体条件就能被识别，从而为协同进化搜索算法中的设计选择提供潜在启示，以提升泛化能力。我们的案例研究采用CCL（Competitive Coevolutionary Learning）并设定固定的变异率$p_m=0.05$作为基线，我们将该基线称为CCL-PM05。之后，我们分别考虑了显式（通过变异 [23]）和隐式（通过选择）两类多样性保持机制。对于显式机制，我们以$0.05$为步长逐步提高变异率，直到$p_m = 0.25$（即CCL-PM25）。对于隐式机制，我们考察了竞争型（CFSD和CFSR）和隐式适应度共享机制 [21]。在为共同进化策略计算适应度的对手采样过程中，可以采用简单随机（CFSR与IFSR）或异质采样方式，后者会在编辑距离的背景下连续选择与前一位对手差异更大的个体（CFSD与IFSD）。同时，Pareto共同进化（PAR）和降低毒力（RV）也在研究范围内。我们针对这些结果开展的详细分析 [15]，包括相关性分析，表明在共同进化学习群体中引入并维持多样性，并不必然导致泛化性能的显著提升。在某些情况下，例如降低毒力的共同进化，并未能明确建立泛化性能与多样性之间的正相关关系。然而，我们观察到，多样性维持（具体来说是隐式适应度共享）促使群体出现了分化（speciated），从而使共同进化后的群体能更好地覆盖策略空间。从[15]的结果可以看出，与基线相比，Best($G_{\mathrm{SPOP}_{U}}$) 和 Avg($G_{\mathrm{SPOP}_{U}}$) 指标无显著差异，但 Ens($G_{\mathrm{SPOP}_{U}}$) 指标明显更优。这是因为对于后者，我们假设存在一种理想的门控机制，始终能够从群体中选择对测试策略表现最佳的个体。对于Pareto共同进化，也能作出类似的观察。当采用有偏测试策略来计算泛化性能时，我们也获得了类似的结果 [15]。我们关于泛化性能的理论与计算研究已验证，对于可以被描述为两方策略博弈的广泛问题类别，任何竞争性共同进化系统都可以进行严格的定量分析。如果这些泛化性能评估能够被谨慎地使用，特别是在明确该性能指的是对测试案例的平均表现时，那么也可以获得有意义的定性结果。这对于分析应用于解决极其复杂现实问题的竞争性共同进化系统具有重要意义。在此类问题中，相关者在起初可能对问题结构理解有限，因此拥有既可进行严格分析、又易于部署的定量工具，对于共同进化系统的实际开发者和设计者来说非常关键，以便能够为具体任务实现合适的学习系统，并做出正确的设计与选择。
在结束本节之前，我们需要指出，虽然关于竞争性协同进化算法（competitive CEAs）的理论研究相较于传统进化算法（EAs）较为有限，但多年来仍取得了显著进展。例如，已经有关于协同优化（cooptimization）的研究，将其作为对经典黑盒优化（black box optimization）的进一步泛化。值得注意的是，早期的理论研究[61]已经在传统优化的一些设置中确立了“No Free Lunch”定理，也就是说，非正式意义上讲，所有算法在所有问题上的平均表现上是相似的。随后，在[62]中的竞争性协同进化环境下，在更受限的自对弈（self-play）设置中，理论上能够成立“Free Lunch”现象。最近，在[50]中，发展了一个在整个问题域所有函数上分析协同优化算法总体性能的理论框架，并进一步细化以涵盖诸如预算约束和搜索历史等实际应用中常见的关键因素。此外，研究[48]还建立了竞争性协同进化算法、协同优化与监督学习之间的紧密关联。

参考文献  
1. Ashlock, D., Kim, E.Y.：指纹识别：囚徒困境策略的可视化与自动分析。IEEE进化计算汇刊，12(5), 647–659 (2008)  
2. Axelrod, R.：迭代囚徒困境中策略的演化。载于L.D. Davis编《遗传算法与模拟退火》第3章，32–41页。Morgan Kaufmann, New York (1987)  
3. Bader-Natal, A., Pollack, J.B.：面向协同进化失败敏感度的度量与可视化。见：协同进化与共同适应系统研讨会，2005年AAAI专题研讨会，Washington DC (2005)  
4. Bari, A.G.：竞争性协同进化算法中的交互式适应度域。博士论文，南佛罗里达大学，佛罗里达 (2019)  
5. Bishop, C.M.：神经网络模式识别。牛津大学出版社，牛津 (1995)  
6. Bucci, A.：协同进化算法中新生的几何结构与信息维度。博士论文，布兰迪斯大学，马萨诸塞 (2007)  
7. Bucci, A., Pollack, J.B.：聚焦与不可传递性——协同进化的几何方面。载于2003年遗传与进化计算大会（GECCO’03）论文集，250-261页 (2003)  
8. Burke, E.K., Gustafson, S., Kendall, G.：遗传编程中的多样性：度量分析及与适应度的相关性。IEEE进化计算汇刊，8(1), 47–62 (2004)  
9. Cartlidge, J., Bullock, S.：通过降低寄生者的毒性来对抗协同进化脱节。进化计算，12(2), 193–222 (2004)  
10. Chellapilla, K., Fogel, D.B.：进化、神经网络、游戏与智能。Proc. IEEE 87(9), 1471–1496 (1999)  
11. Chong, S.Y., Tan, M.K., White, J.D.：观察神经网络演化学习黑白棋游戏。IEEE进化计算汇刊，9(3), 240–251 (2005)  
12. Chong, S.Y., Tiˇno, P., He, J., Yao, X.：协同进化系统分析的新框架——有向图表示及随机游走。进化计算，27(2), 195–228 (2019)  
13. Chong, S.Y., Tiˇno, P., Ku, D.C., Yao, X.：提升协同进化学习泛化性能。IEEE进化计算汇刊，16(1), 70–85 (2012)  
14. Chong, S.Y., Tiˇno, P., Yao, X.：协同进化学习中泛化性能的度量。IEEE进化计算汇刊，12(4), 479–505 (2008)  
15. Chong, S.Y., Tiˇno, P., Yao, X.：协同进化学习中泛化与多样性的关系。IEEE计算智能与人工智能游戏汇刊，1(3), 214–232 (2009)  
16. Chong, S.Y., Yao, X.：迭代囚徒困境中的行为多样性、选择与噪声。IEEE进化计算汇刊，9(6), 540–551 (2005)  
17. Chung, K.L.：概率论教程，第三版。学术出版社，圣地亚哥 (2001)  
18. Cliff, D., Miller, G.F.：追踪红皇后：协同进化仿真中的适应性进步测量。载于人工生命进展：第三届欧洲人工生命大会论文集，Lecture Notes in Computer Science, 卷929, 200–218页，Springer, Berlin (1995)  
19. Darwen, P.J.：通过物种划分自动模块化的协同进化学习。博士论文，新南威尔士大学，悉尼 (1996)  
20. Darwen, P.J., Yao, X.：迭代囚徒困境中鲁棒策略的演化。见《进化计算进展》，Lecture Notes in Artificial Intelligence, 卷956, 276–292页，Springer, Berlin (1995)  
21. Darwen, P.J., Yao, X.：每种分群方法都有其分群：适应度分享与隐式适应度分享的比较。见第四届自然并行问题求解国际会议PPSN IV, Lecture Notes in Computer Science, 卷1141, 398–407页，Springer, Berlin (1996)  
22. Darwen, P.J., Yao, X.：物种划分作为自动范畴化模块化。IEEE进化计算汇刊，1(2), 101–108 (1997)  
23. Darwen, P.J., Yao, X.：是否额外的遗传多样性能维持协同进化军备竞赛中的升级。国际知识驱动智能工程系统杂志，4(3), 191–200 (2000)  
24. Darwen, P.J., Yao, X.：在中等水平合作下的迭代囚徒困境中的协同进化：导弹防御系统应用。国际计算智能应用杂志，2(1), 83–107 (2002)  
25. Darwen, P.J., Yao, X.：为什么更多的选择会导致迭代囚徒困境中的合作变少。见2001年IEEE进化计算大会（CEC’01）论文集，987–994页，首尔，韩国 (2002)  
26. de Jong, E.D., Pollack, J.B.：从协同进化中获得理想评估。进化计算，12(2), 159–192 (2004)  
27. Ekárt, A., Németh, S.：保持遗传程序的多样性。见第五届欧洲遗传编程会议EuroGP 02, Lecture Notes in Computer Science, 卷2278, 162–171页，Springer, Berlin (2002)  
28. Ficici, S.G.：协同进化算法中的解概念。博士论文，布兰迪斯大学，马萨诸塞 (2004)  
29. Ficici, S.G., Melnik, O., Pollack, J.B.：协同进化中选择方法的博弈论及动力系统分析。IEEE进化计算汇刊，9(6), 580–602 (2005)  
30. Ficici, S.G., Pollack, J.B.：协同进化学习中的帕累托最优性。见人工生命进展：第六届欧洲人工生命大会（ECAL’01），Lecture Notes in Computer Science, 卷2159, 316–325页，Springer, Prague (2001)  
31. Floreano, D., Nolﬁ, S.：上帝保佑红皇后！协同进化机器人中的竞争。见J.R. Koza, K. Deb, M. Dorigo, D.B. Fogel, M. Garzon, H. Iba, R.L. Riolo编《遗传编程1997：第二届年会论文集》，398–406页，Morgan Kaufmann, 斯坦福大学，加利福尼亚，美国 (1997)
